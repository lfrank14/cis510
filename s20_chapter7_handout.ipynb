{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"s20_chapter7_handout.ipynb","provenance":[{"file_id":"1ykbN6M3eQxhntPImkYQL4zBg8VH4zCXK","timestamp":1589326203269},{"file_id":"1WF9dX_JcOZOS9Dh3Rr3GeRYkV-nisBKJ","timestamp":1589044058483},{"file_id":"1D_rVwtN6bpVFhzNq50yIyMGJ8zzwXtVR","timestamp":1588092424463},{"file_id":"1tW_0spm8E_mzURIjBxPKWFHbsvVcpey8","timestamp":1581894983969},{"file_id":"1suSjmwRopopnvO3L3xDxDtdllZpTM1zp","timestamp":1556465823183}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fqYN2skxBOcs"},"source":["<center><h1>Chapter 7 - Word vectors and word embeddings</h1></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zzuOzBrABOcu"},"source":["First, I want to again give a shoutout to Allison Parrish (http://www.decontextualize.com/). She mixes machine learning with poetry (!) I really like her work and explanations. I borrowed her color mixing example.\n","\n","The topic I want to take up this week is something called \"embeddings\". The general idea is to take some complex structure and reduce it to a vector encoding. And the goal is for the vector to be low_dimensional and compact (non-sparse). So in the hundreds range, not the thousands range. And most importantly, the vector captures \"features\" of the structure in a way that structures that are similar have similar vectors.\n","\n","Our interest is in text so we will look at how to \"embed\" text into a vector. But the applications of embedding are much broader. In our department for example, researchers are attempting to find embeddings for social networks: ways to encode a complex graph structure into a vector. I have seen something similar in Biology, taking Biological networks and reducing them to a vector representation. In fact, looking at your majors, I believe I can find researchers in each looking at embeddings.\n","\n","One of the attributes of embeddings is that we can use similarity functions on the vectors. Thus we can find social networks that are similar, biological networks that are similar, students that are similar. We will see that with words that are similar in this chapter."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n-AAgxmxBOcv"},"source":["#1. Animal similarity\n","\n","I'm going to kind of sneak up on the word embedding idea.\n","We'll begin by considering a small subset of English: words for animals. Our task will be to  find similarities among these words and the creatures they designate. So we will look for similarities between `kitten` and `hamster`. I will come up with 2 features that I think are important for animals. See below:\n","\n","![Animal spreadsheet](http://static.decontextualize.com/snaps/animal-spreadsheet.png)\n","\n","This spreadsheet associates a handful of animals with two numbers: their cuteness and their size, both in a range from zero to one hundred. The values themselves are simply based on personal judgment. Your taste in cuteness and evaluation of size may differ significantly from mine. As with all data, these data are simply a mirror reflection of the person who collected them. In other words, always have to be aware of human bias in problems like this.\n","\n","This is a step toward what is called a word-vector. For instance the word `kitten` has a 2d vector 95,15.\n","\n","Let's build the pandas version of the table shown. I'm going to use a pandas method that will convert a list of row values into a table.\n"]},{"cell_type":"code","metadata":{"id":"lSGjHwKzrDJi","colab_type":"code","colab":{}},"source":["rows = [\n","    ['kitten', 95,15],\n","    ['hamster', 80,8],\n","    ['tarantula', 8,3],\n","    ['puppy', 90, 20],\n","    ['crocodile', 5, 40],\n","    ['dolphin', 60,45],\n","    ['panda bear', 75, 40],\n","    ['lobster', 2, 15],\n","    ['capybara', 70, 30],\n","    ['elephant', 65, 90],\n","    ['mosquito', 1, 1],\n","    ['goldfish', 25, 2],\n","    ['horse', 50, 50],\n","    ['chicken', 25, 15]\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJPlNOUUz1cC","colab_type":"code","colab":{}},"source":["columns = ['animal', 'cuteness', 'size']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_bjpk6xRsZCB","colab_type":"code","outputId":"2ec85da6-7078-4b22-fd99-e908b15f6c02","executionInfo":{"status":"ok","timestamp":1589408905585,"user_tz":420,"elapsed":2019,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":514}},"source":["import pandas as pd\n","animal_table = pd.DataFrame.from_records(rows, columns=columns)  #provide rows and column names\n","animal_table = animal_table.set_index(['animal'])  #make animal column the index\n","animal_table"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cuteness</th>\n","      <th>size</th>\n","    </tr>\n","    <tr>\n","      <th>animal</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>kitten</th>\n","      <td>95</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>hamster</th>\n","      <td>80</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>tarantula</th>\n","      <td>8</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>puppy</th>\n","      <td>90</td>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>crocodile</th>\n","      <td>5</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>dolphin</th>\n","      <td>60</td>\n","      <td>45</td>\n","    </tr>\n","    <tr>\n","      <th>panda bear</th>\n","      <td>75</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>lobster</th>\n","      <td>2</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>capybara</th>\n","      <td>70</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>elephant</th>\n","      <td>65</td>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>mosquito</th>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>goldfish</th>\n","      <td>25</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>horse</th>\n","      <td>50</td>\n","      <td>50</td>\n","    </tr>\n","    <tr>\n","      <th>chicken</th>\n","      <td>25</td>\n","      <td>15</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            cuteness  size\n","animal                    \n","kitten            95    15\n","hamster           80     8\n","tarantula          8     3\n","puppy             90    20\n","crocodile          5    40\n","dolphin           60    45\n","panda bear        75    40\n","lobster            2    15\n","capybara          70    30\n","elephant          65    90\n","mosquito           1     1\n","goldfish          25     2\n","horse             50    50\n","chicken           25    15"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"hAJJP6swrBuX","colab_type":"text"},"source":["\n","The values in the table give us a way to make determinations about which animals are similar. For example, try to answer the following question:\n","\n"," *Which animal is most similar to a `capybara`?*\n"," \n","  You could go through the values one by one and use euclidean distance (or cosine similarity) to make that evaluation. This is quite similar to what we were doing with `ordered_distances`.\n","\n","Let's try visualizing the data as points in 2-dimensional space:\n","\n","![Animal space](http://static.decontextualize.com/snaps/animal-space.png)\n"]},{"cell_type":"markdown","metadata":{"id":"WbOTItEbqBO4","colab_type":"text"},"source":["##Bring in puddles now\n","\n","We need to use Euclidean distance."]},{"cell_type":"code","metadata":{"id":"j_GdU8ACeitW","colab_type":"code","outputId":"04af2711-2235-4b7b-88d1-f91b117b898d","executionInfo":{"status":"ok","timestamp":1589408906548,"user_tz":420,"elapsed":2972,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#flush the old directory\n","!rm -r  'uo_puddles'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["rm: cannot remove 'uo_puddles': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rPISZH5nclhU","colab_type":"code","colab":{}},"source":["my_github_name = 'uo-puddles'  #replace with your account name"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NuaxZXKXcrNL","colab_type":"code","colab":{}},"source":["#clone_url = f'https://github.com/{my_github_name}/w20_ds_library.git'\n","clone_url = f'https://github.com/{my_github_name}/uo_puddles.git'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Rmp86ySdkr4z","outputId":"6dea0309-768c-43ce-e858-d0b1dced627b","executionInfo":{"status":"ok","timestamp":1589408908141,"user_tz":420,"elapsed":4539,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["#get the latest.\n","!git clone $clone_url \n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'uo_puddles'...\n","remote: Enumerating objects: 231, done.\u001b[K\n","remote: Counting objects: 100% (231/231), done.\u001b[K\n","remote: Compressing objects: 100% (195/195), done.\u001b[K\n","remote: Total 231 (delta 137), reused 64 (delta 33), pack-reused 0\u001b[K\n","Receiving objects: 100% (231/231), 58.17 KiB | 9.69 MiB/s, done.\n","Resolving deltas: 100% (137/137), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9r8-rTeMkr5C","colab":{}},"source":["import uo_puddles.uo_puddles as up"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uLl2huwYtj_b","colab_type":"text"},"source":["It looks to me like capybara is closest to panda. But given a graph representation, I can actually put a number on that \"closeness\".\n"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1589408908373,"user_tz":420,"elapsed":4754,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"id":"8IJjJNywBOc1","outputId":"7fd4f272-42d0-4fa7-d964-def327b46a4e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["up.euclidean_distance([70, 30], [75, 40]) # panda and capybara  11.180339887498949"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11.180339887498949"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5qmw4wFLBOc8"},"source":["Looking again at the graph, \"tarantula\" and \"elephant\" look far away. Again, we can put a number on this."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1589408908373,"user_tz":420,"elapsed":4741,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"id":"IHBJ2l61BOc9","outputId":"7e75eb3c-77f0-4bd1-d004-95e914b23e15","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["up.euclidean_distance([8, 3], [65, 90]) # tarantula and elephant  104.0096149401583"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["104.0096149401583"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fsO1_hxpBOdB"},"source":["Modeling animals in this way has interesting properties. For example, you can pick an arbitrary point in \"animal space\" and then find the animal closest to that point. If you imagine an animal of size 25 and cuteness 30, you can easily look at the space to find the animal that most closely fits that description: the chicken.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7OcD3PEm5_Yq","colab_type":"text"},"source":["I am going to write a special function to work with the animal table. I could probably rework my existing `ordered_distances` function, but decided easier to write this new one."]},{"cell_type":"code","metadata":{"id":"tV5wf3sKqtfV","colab_type":"code","colab":{}},"source":["def ordered_embeddings(target_vector, table):\n","  names = table.index.tolist()\n","  ordered_list = []\n","  for i in range(len(names)):\n","    name = names[i]\n","    row = table.loc[name].tolist()\n","    d = up.euclidean_distance(target_vector, row)\n","    ordered_list.append([d, names[i]])\n","  ordered_list = sorted(ordered_list)\n","\n","  return ordered_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6TPlN_zBxB_q","colab_type":"code","outputId":"a41d9805-3126-468a-9be4-1f663ef1ed26","executionInfo":{"status":"ok","timestamp":1589408908374,"user_tz":420,"elapsed":4726,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["pup = animal_table.loc['puppy'].tolist()\n","\n","ordered_embeddings(pup, animal_table)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0.0, 'puppy'],\n"," [7.0710678118654755, 'kitten'],\n"," [15.620499351813308, 'hamster'],\n"," [22.360679774997898, 'capybara'],\n"," [25.0, 'panda bear'],\n"," [39.05124837953327, 'dolphin'],\n"," [50.0, 'horse'],\n"," [65.19202405202648, 'chicken'],\n"," [67.446274915669, 'goldfish'],\n"," [74.33034373659252, 'elephant'],\n"," [83.74365647617735, 'tarantula'],\n"," [87.32124598286491, 'crocodile'],\n"," [88.14193099768123, 'lobster'],\n"," [91.00549433962765, 'mosquito']]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"g1CoTvMWqqkf","colab_type":"text"},"source":["Let's look at it  geometrically. You can  answer questions like: what's halfway between a chicken and an elephant? Simply draw a line from \"elephant\" to \"chicken,\" mark off the midpoint and find the closest animal. (According to our chart, halfway between an elephant and a chicken is a horse.) Let's check that out computationally.\n","\n"]},{"cell_type":"code","metadata":{"id":"bWPggzT8zAsv","colab_type":"code","outputId":"8f5dcb5b-8b48-4aea-df2f-dae64f235324","executionInfo":{"status":"ok","timestamp":1589408908375,"user_tz":420,"elapsed":4713,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["elephant = animal_table.loc['elephant'].tolist()\n","chicken = animal_table.loc['chicken'].tolist()\n","half_vector = [(e+c)/2  for e,c in zip(elephant, chicken)]  #using fancy list-building version\n","half_vector  #[45.0, 52.5]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[45.0, 52.5]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"QW5G6GCpKqoV","colab_type":"text"},"source":["Sorry, I am using a fancy version of my new list from old list gist. I could have written a loop, but wanted a one-liner.\n","<pre>\n","half_vector = [(e+c)/2  for e,c in zip(elephant, chicken)]\n","</pre>\n","I would not worry too much about it for now. I am going to ask you to write a function that does the same in just a bit."]},{"cell_type":"code","metadata":{"id":"zD1g7ypc0VH9","colab_type":"code","outputId":"7f4ee72a-8a7b-4809-8e34-a7da160039d0","executionInfo":{"status":"ok","timestamp":1589408908375,"user_tz":420,"elapsed":4703,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["ordered_embeddings(half_vector, animal_table)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[5.5901699437494745, 'horse'],\n"," [16.77050983124842, 'dolphin'],\n"," [32.5, 'panda bear'],\n"," [33.63406011768428, 'capybara'],\n"," [41.907636535600524, 'crocodile'],\n"," [42.5, 'chicken'],\n"," [42.5, 'elephant'],\n"," [54.31620384378864, 'goldfish'],\n"," [55.509008277936296, 'puppy'],\n"," [56.61492736019362, 'hamster'],\n"," [57.05479822065801, 'lobster'],\n"," [61.80008090609591, 'tarantula'],\n"," [62.5, 'kitten'],\n"," [67.73662229547618, 'mosquito']]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"Xu3bbkfNy6aT","colab_type":"text"},"source":["You can also ask: what's the *difference* between a hamster and a tarantula? According to our plot, it's about seventy five units of cute (and a few units of size).\n","\n","The relationship of \"difference\" is an interesting one, because it allows us to reason about *analogous* relationships. In the chart below, I've drawn an arrow from \"tarantula\" to \"hamster\" (in blue):\n","\n","![Animal analogy](http://static.decontextualize.com/snaps/animal-space-analogy.png)\n","\n","You can understand this arrow as being the *relationship* between a tarantula and a hamster, in terms of their size and cuteness (i.e., hamsters and tarantulas are about the same size, but hamsters are much cuter). In the same diagram, I've also transposed this same arrow (this time in red) so that its origin point is \"chicken.\" The arrow ends closest to \"kitten.\" What we've discovered is that the animal that is about the same size as a chicken but much cuter is... a kitten. To put it in terms of an analogy:\n","\n","    Tarantulas are to hamsters as chickens are to kittens.\n","    \n"]},{"cell_type":"code","metadata":{"id":"pkwYUrce1CnO","colab_type":"code","outputId":"4cce8569-b9c0-4960-ebbc-ad67bc141f51","executionInfo":{"status":"ok","timestamp":1589408908376,"user_tz":420,"elapsed":4694,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tarantula = animal_table.loc['tarantula'].tolist()\n","hamster = animal_table.loc['hamster'].tolist()\n","thd = up.euclidean_distance(tarantula, hamster)  #tarantula hamster distance\n","thd  #72.17340230306452"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["72.17340230306452"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"vxRAV6_FPa9r","colab_type":"text"},"source":["Now get animal distances from chicken and find the one closest to 72.17."]},{"cell_type":"code","metadata":{"id":"sGPM37A71dur","colab_type":"code","outputId":"5dbabc92-5702-40ee-a672-fbeaa42de8e8","executionInfo":{"status":"ok","timestamp":1589408908376,"user_tz":420,"elapsed":4682,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":255}},"source":["chick_ds = ordered_embeddings(chicken, animal_table)\n","chick_ds"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0.0, 'chicken'],\n"," [13.0, 'goldfish'],\n"," [20.808652046684813, 'tarantula'],\n"," [23.0, 'lobster'],\n"," [27.784887978899608, 'mosquito'],\n"," [32.01562118716424, 'crocodile'],\n"," [43.01162633521314, 'horse'],\n"," [46.09772228646444, 'dolphin'],\n"," [47.43416490252569, 'capybara'],\n"," [55.44366510251645, 'hamster'],\n"," [55.90169943749474, 'panda bear'],\n"," [65.19202405202648, 'puppy'],\n"," [70.0, 'kitten'],\n"," [85.0, 'elephant']]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"SMm_N-Hn3b0Q","colab_type":"code","outputId":"ecad5d23-8492-407a-9837-89730c16d5f7","executionInfo":{"status":"ok","timestamp":1589408908377,"user_tz":420,"elapsed":4669,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["min(chick_ds, key=lambda x:abs(x[0]-thd))  #('kitten', 70.0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[70.0, 'kitten']"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"JvX14fRrdKT4","colab_type":"text"},"source":["The above code looks kind of complex. Normally you would give the min function a list of values and it would find the smallest. But you can also give it a 2nd, optional argument: how you want the comparison to be made. So you have:\n","\n","<pre>\n","key=lambda x:abs(x[0]-thd)\n","</pre>\n","\n","In words, we want the minimum value when looking at the distance (i.e., x[0]) minus 72.17 (i.e., thd). And by the way, take the absolute value of that difference. So this should find the pair who has a distance from chicken that is the closest to 72.17.\n","\n","Kind of a brief explanation, right?. But I am hoping you will just go with it for now. I won't expect you to come up with this kind of code in your own programs. We could use a loop to do it, but it would be kind of complex. Check it out.\n"]},{"cell_type":"code","metadata":{"id":"2ifN9-zHNVnW","colab_type":"code","outputId":"103304c7-a8de-4f18-c61f-f72cfac54817","executionInfo":{"status":"ok","timestamp":1589408908377,"user_tz":420,"elapsed":4659,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#loop equiv of min(chick_ds, key=lambda x:abs(x[0]-thd))  #('kitten', 70.0)\n","\n","d1, n1 = chick_ds[1]  #just to get us started\n","the_min = [abs(d1-thd), 1]\n","for i in range(2,len(chick_ds)):\n","  d,n = chick_ds[i]\n","  chick_diff = abs(d-thd)\n","  if chick_diff < the_min[0]:\n","    the_min = [chick_diff, i]\n","\n","the_winner = chick_ds[the_min[1]]\n","the_winner"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[70.0, 'kitten']"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jWi-8rZtBOdD"},"source":["#2. Language with vectors: colors\n","\n","So far, so good. We have a system in place—albeit highly subjective—for talking about animals and the words used to name them. I want to talk about another vector space that has to do with language: the vector space of colors.\n","\n","Colors are often represented in computers as vectors with three dimensions: red, green, and blue. Just as with the animals in the previous section, we can use these vectors to answer questions like: which colors are similar? What's the most likely color name for an arbitrarily chosen set of values for red, green and blue? Given the names of two colors, what's the name of those colors' \"average\"?\n","\n","We'll be working with this [color data](https://github.com/dariusk/corpora/blob/master/data/colors/xkcd.json) from the [xkcd color survey](https://blog.xkcd.com/2010/05/03/color-survey-results/). The data relates a color name to the RGB value associated with that color. [Here's a page that shows what the colors look like](https://xkcd.com/color/rgb/).\n","\n","I've put it in a table for us."]},{"cell_type":"code","metadata":{"id":"m9PN8vtBbIEy","colab_type":"code","colab":{}},"source":["url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vT7WNqkgfIYL5AWgb8aGSGhvh3wo-JwTQlJzN1Y2LYH09fzLtfeKHMDau9s6PcOBwU01-DfbPuEzhTZ/pub?output=csv'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WmdDZzHBbTMq","colab_type":"code","colab":{}},"source":["color_table = pd.read_csv(url, encoding='utf-8', dtype={'color':str}, na_filter=False)\n","color_table = color_table.set_index(['color'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcPCKWhVb33R","colab_type":"code","outputId":"b97f2451-14db-4b97-8724-3125afeb9acd","executionInfo":{"status":"ok","timestamp":1589408908749,"user_tz":420,"elapsed":5010,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["color_table.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hex</th>\n","      <th>red</th>\n","      <th>green</th>\n","      <th>blue</th>\n","    </tr>\n","    <tr>\n","      <th>color</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>acid green</th>\n","      <td>#8ffe09</td>\n","      <td>143</td>\n","      <td>254</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>adobe</th>\n","      <td>#bd6c48</td>\n","      <td>189</td>\n","      <td>108</td>\n","      <td>72</td>\n","    </tr>\n","    <tr>\n","      <th>algae</th>\n","      <td>#54ac68</td>\n","      <td>84</td>\n","      <td>172</td>\n","      <td>104</td>\n","    </tr>\n","    <tr>\n","      <th>algae green</th>\n","      <td>#21c36f</td>\n","      <td>33</td>\n","      <td>195</td>\n","      <td>111</td>\n","    </tr>\n","    <tr>\n","      <th>almost black</th>\n","      <td>#070d0d</td>\n","      <td>7</td>\n","      <td>13</td>\n","      <td>13</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  hex  red  green  blue\n","color                                  \n","acid green    #8ffe09  143    254     9\n","adobe         #bd6c48  189    108    72\n","algae         #54ac68   84    172   104\n","algae green   #21c36f   33    195   111\n","almost black  #070d0d    7     13    13"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gI8yz-tkBOdL"},"source":["The following function converts colors from hex format (`#1a2b3c`) to a tuple of integers:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zs4nhn46BOdM","colab":{}},"source":["def hex_to_int(s:str) -> tuple:\n","  assert isinstance(s, str), f's must be a string but is instead a {type(s)}'\n","  assert len(s) == 7, f's must be 7 characters long but is instead {len(s)}'\n","  assert s[0] == '#', f's must start with a # but instead starts with an {s[0]}'\n","\n","  s = s.lstrip(\"#\")  #strip # off of left-hand side of string\n","  red = int(s[:2], 16)\n","  green = int(s[2:4], 16)\n","  blue = int(s[4:6], 16)\n","  return [red, green, blue]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ax-MwS2VsyY4","colab_type":"text"},"source":["I added type hints and asserts to help you remember what the function expects as input."]},{"cell_type":"code","metadata":{"id":"y0vLRf1wQwaT","colab_type":"code","outputId":"84a90be9-e481-4f09-8d15-550af4bd2db6","executionInfo":{"status":"ok","timestamp":1589408908750,"user_tz":420,"elapsed":4995,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["hex_to_int('#8ffe09')  #[143, 254, 9] - matches what we see in table"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[143, 254, 9]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"ngyF69H1s-uq","colab_type":"text"},"source":["##What the heck is \"hex\"?\n","\n","Computer scientists (and color mixers!) like to play around with different number bases. We all know base 10 to include the digits 0-9. Well, hex (hexidecimal) is base 16. It includes the digits 0-f. I suppose if we had to work with aliens that had 8 fingers per hand, we would have to get good at hex arithmetic. It so happens that the computer often stores data in 16 base format as well. It typically does the hex to decimal translation for you. But not always :)\n","\n","Python has a function for converting a hex number (as a string) into a decimal equivalent. Check it out."]},{"cell_type":"code","metadata":{"id":"CCfBXctYt5y2","colab_type":"code","outputId":"6b29966e-91bb-4cd1-cd9e-9b0577d83bd0","executionInfo":{"status":"ok","timestamp":1589408908751,"user_tz":420,"elapsed":4980,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["int('8f', 16)  #give decimal equiv of 8f in base 16, i.e., hex"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["143"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"9Y4vU8lQuaKm","colab_type":"text"},"source":["We can go the other way. You can ignore the 0x prefix."]},{"cell_type":"code","metadata":{"id":"qAOg_cKJuPR8","colab_type":"code","outputId":"81f1faff-fb7d-4b36-de33-9f9dfefeb1b0","executionInfo":{"status":"ok","timestamp":1589408908751,"user_tz":420,"elapsed":4966,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["hex(143)  #give hex version of decimal 143"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'0x8f'"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"qyvFZvN7SV-r","colab_type":"text"},"source":["We won't be using hex going forward but thought you might like to know what it is, at least at a high level."]},{"cell_type":"markdown","metadata":{"id":"xe3mkobBB07T","colab_type":"text"},"source":["##Check out cosine differences\n","\n","Reminder: cosine ranges from 0 to 1 with 1 being an exact match."]},{"cell_type":"markdown","metadata":{"id":"lrNuXpnI2bFG","colab_type":"text"},"source":["##Wrangling: drop hex column\n","\n","We don't need it. We already have the equivalent red, green, and blue values. And btw, these are in decimal."]},{"cell_type":"code","metadata":{"id":"DgoSoCEp2isT","colab_type":"code","outputId":"e8c9ebb7-0f26-4894-f698-4acecd764932","executionInfo":{"status":"ok","timestamp":1589408908752,"user_tz":420,"elapsed":4951,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":235}},"source":["color_table = color_table.drop(['hex'], axis=1) #need axis=1 to say we are dropping a column\n","color_table.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>red</th>\n","      <th>green</th>\n","      <th>blue</th>\n","    </tr>\n","    <tr>\n","      <th>color</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>acid green</th>\n","      <td>143</td>\n","      <td>254</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>adobe</th>\n","      <td>189</td>\n","      <td>108</td>\n","      <td>72</td>\n","    </tr>\n","    <tr>\n","      <th>algae</th>\n","      <td>84</td>\n","      <td>172</td>\n","      <td>104</td>\n","    </tr>\n","    <tr>\n","      <th>algae green</th>\n","      <td>33</td>\n","      <td>195</td>\n","      <td>111</td>\n","    </tr>\n","    <tr>\n","      <th>almost black</th>\n","      <td>7</td>\n","      <td>13</td>\n","      <td>13</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              red  green  blue\n","color                         \n","acid green    143    254     9\n","adobe         189    108    72\n","algae          84    172   104\n","algae green    33    195   111\n","almost black    7     13    13"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Fp1FmHl7B5YS","outputId":"cf550878-76a2-4ea9-cd3e-889208487393","executionInfo":{"status":"ok","timestamp":1589408908752,"user_tz":420,"elapsed":4935,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["olive_vector = color_table.loc['olive'].tolist()\n","olive_vector\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[110, 117, 14]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"42ca8f56-e15a-4789-8859-3cef33009cea","executionInfo":{"status":"ok","timestamp":1589408908753,"user_tz":420,"elapsed":4921,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"id":"o29LmxzWyeIb","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["red_vector = color_table.loc['red'].tolist()\n","red_vector\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[229, 0, 0]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"GGJPlABEyaR0","colab_type":"code","outputId":"79bbdf86-04d1-49ef-da8b-be98acbc0906","executionInfo":{"status":"ok","timestamp":1589408908753,"user_tz":420,"elapsed":4906,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["up.cosine_similarity(olive_vector, red_vector)  #0.6823879113063314"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6823879113063314"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"cEIYRGQ128At","colab_type":"text"},"source":["##Closest colors"]},{"cell_type":"code","metadata":{"id":"9523tyC13Bov","colab_type":"code","outputId":"d79878a0-36ab-4a6c-a402-7228af1bd3c2","executionInfo":{"status":"ok","timestamp":1589408909097,"user_tz":420,"elapsed":5238,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["ordered_embeddings(red_vector, color_table)[:10]  #closest colors to red"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0.0, 'red'],\n"," [25.079872407968907, 'fire engine red'],\n"," [29.068883707497267, 'bright red'],\n"," [45.552167895721496, 'tomato red'],\n"," [45.73838650411709, 'cherry red'],\n"," [46.33573135281238, 'scarlet'],\n"," [53.563046963368315, 'vermillion'],\n"," [56.2672195865408, 'orangish red'],\n"," [56.49778756730214, 'cherry'],\n"," [59.84981202978001, 'lipstick red']]"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gvUfJaZHBOdo"},"source":["#Assignment 1\n","<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n","\n","I'd like to start messing around with \"color arithmetic\". I think you will see it is kind of cool. I'll ask you to answer questions like this:\n","\n","* What color do you get by subtracting \"red\" from \"purple\"?\n","\n","* What's blue plus green?\n","\n","* Find the average of black and white?\n","\n","* An analogy: pink is to red as X is to blue.\n","\n","* Another analogy: Navy is to blue as X is to green.\n","\n","I am going to ask you to write a collection of functions to help answer these questions. I'll give you the first below. I am going to include type hints and asserts. **YOU DO NOT NEED TO INCLUDE THESE IN YOUR FUNCTIONS.** They can be intimidating when just getting your feet wet. But feel free to try your hand if you are feeling adventurous.\n","\n","Here you go. A function for subtracting 2 vectors of any length as long as they are of equal length.\n","\n","You can also see that I am giving you the one-line version of the function body as a comment. You can ignore it, no problem. But if you can get your head around it, I think it is a safer approach. The less lines of code you have to write, the less places to introduce bugs."]},{"cell_type":"code","metadata":{"id":"zSDmHKuBVShY","colab_type":"code","colab":{}},"source":["def subtractv(x:list, y:list) -> list:\n","  assert isinstance(x, list), f\"x must be a list but instead is {type(x)}\"\n","  assert isinstance(y, list), f\"y must be a list but instead is {type(y)}\"\n","  assert len(x) == len(y), f\"x and y must be the same length\"\n","\n","  #result = [(c1 - c2) for c1, c2 in zip(x, y)]  #one-line compact version - called a list comprehension\n","\n","  result = []\n","  for i in range(len(x)):\n","    c1 = x[i]\n","    c2 = y[i]\n","    result.append(c1-c2)\n","\n","  return result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yEKPSOeX1jCe","colab_type":"code","outputId":"55d02e71-ab1b-4d36-f922-c04cbdce18c4","executionInfo":{"status":"ok","timestamp":1589408909098,"user_tz":420,"elapsed":5220,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#list comprehension\n","\n","y = [1,2,3]\n","x = [item+1 for item in y]  #same as below\n","z = [y[i]+1   for i in range(len(y))]\n","z\n","x"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2, 3, 4]"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"No6eFaOQ-f7M","colab_type":"code","outputId":"6b11799d-cea5-4e94-f7b8-9880d48c1c0b","executionInfo":{"status":"ok","timestamp":1589408909099,"user_tz":420,"elapsed":5207,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["subtractv([5, 10, 20],[1, 2, 3])  #[4, 8, 17]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[4, 8, 17]"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TouLvo7W-46r"},"source":["##Step 1.\n","\n","Please define addv. Use subtractv as a template."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YNFOFCHU8xCk","colab":{}},"source":["#your code\n","def addv(x:list, y:list) -> list:\n","  assert isinstance(x, list), f\"x must be a list but instead is {type(x)}\"\n","  assert isinstance(y, list), f\"y must be a list but instead is {type(y)}\"\n","  assert len(x) == len(y), f\"x and y must be the same length\"\n","\n","  result = [(c1+c2) for c1, c2 in zip(x,y)]\n","  return result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YL1iFiJa-46w","outputId":"176174b6-7569-4d84-8f81-4e2f19f5defd","executionInfo":{"status":"ok","timestamp":1589408909100,"user_tz":420,"elapsed":5188,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["addv([5, 10, 20],[1, 2, 3])  #[6, 12, 23]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[6, 12, 23]"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s4YIptwY-LEb"},"source":["##Step 2.\n","\n","Please define dividev. This function takes a list and a number and divides every element of the list by the number."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OD3cHKNQ-LEb","colab":{}},"source":["#your code\n","def dividev(x:list, y:int) -> list:\n","  assert isinstance(x, list), f\"x must be a list but instead is {type(x)}\"\n","  assert isinstance(y, int), f\"y must be a list but instead is {type(y)}\"\n","\n","  result = [c/y for c in x]\n","  return result"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"poQjvnlC-LEh","outputId":"a925f8d1-acb0-4afe-acae-7487de6cd379","executionInfo":{"status":"ok","timestamp":1589408909268,"user_tz":420,"elapsed":5336,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["dividev([2, 10, 20], 2)  #[1.0, 5.0, 10.0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1.0, 5.0, 10.0]"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wniz-Ipn_Xgj"},"source":["##Step 3.\n","\n","This one is move challenging. I would like the mean vector from a matrix. As reminder, a matrix is a list of vectors. So add all the vectors up then divide each element by the length of the matrix. Please use `addv` and `dividev` in your function body."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RkCzg2MhBOd6","colab":{}},"source":["#your code\n","def meanv(matrix:list) -> list:\n","  assert isinstance(matrix, list), f\"x must be a list but instead is {type(x)}\"\n","  assert len(matrix) >= 1, f\"matrix must have at least one row\"\n","  \n","  #for m in range(len(x)-1): \n","   # if m<1:\n","   #   total = addv(x[m],x[m+1])\n","   # else:\n","   #  total = addv(total,x[m+1])\n","  #result = dividev(total,len(x))\n","\n","# another solution would be to start with sumv = first row of matrix then loop through the rest of the rows and adding them together\n","  sumv = matrix[0]\n","  for row in matrix[1:]:\n","    sumv = addv(sumv,row)\n","  result = dividev(sumv, len(matrix))\n","\n","# another way would be to transpose the matrix and then sum it   \n","\n","\n","  \n","\n","  return result\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nodrozbt_u_w","colab_type":"code","outputId":"fa49c351-e61c-40df-f123-279f96116de4","executionInfo":{"status":"ok","timestamp":1589408909270,"user_tz":420,"elapsed":5315,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["test = [[0, 1], [2, 2], [4, 3]]  #test matrix\n","test\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0, 1], [2, 2], [4, 3]]"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"6CSbmooO_pWT","colab_type":"code","outputId":"416a4e0a-33ae-4c10-8724-bde66c3e5585","executionInfo":{"status":"ok","timestamp":1589408909270,"user_tz":420,"elapsed":5300,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["meanv(test)  #[2.0, 2.0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2.0, 2.0]"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"w5HzK5Q-zw9F","colab_type":"text"},"source":["##Step 4.\n","\n","Please answer this question:\n","\n","Find the ten colors closest to the average of 'black' and 'white'.\n","\n"]},{"cell_type":"code","metadata":{"id":"nCoSoSkj0DyW","colab_type":"code","outputId":"b3bad93d-7a88-4873-d027-d77149ee19bc","executionInfo":{"status":"ok","timestamp":1589408909271,"user_tz":420,"elapsed":5287,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["#your code\n","black_v = color_table.loc['black'].to_list()\n","white_v = color_table.loc['white'].to_list()\n","\n","ordered_embeddings(meanv([black_v,white_v]), color_table)[:10]\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[4.330127018922194, 'medium grey'],\n"," [18.567444627627143, 'purple grey'],\n"," [19.716744153130353, 'steel grey'],\n"," [21.511624764298954, 'battleship grey'],\n"," [22.46664193866097, 'grey purple'],\n"," [24.14021540914662, 'purplish grey'],\n"," [24.264171117101856, 'greyish purple'],\n"," [25.470571253900058, 'steel'],\n"," [26.12948526090784, 'warm grey'],\n"," [26.205915362757317, 'green grey']]"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cOrBWQiC0J1N"},"source":["##Step 5.\n","\n","Please answer this question:\n","\n","Find the 10 colors closest to what you get by subtracting \"red\" from \"purple\".\n"]},{"cell_type":"code","metadata":{"id":"720AxXaA0dlR","colab_type":"code","outputId":"46b79460-ece3-431c-fe84-532088ee5692","executionInfo":{"status":"ok","timestamp":1589408909596,"user_tz":420,"elapsed":5597,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["#your code\n","red_v = color_table.loc['red'].to_list()\n","purple_v = color_table.loc['purple'].to_list()\n","\n","ordered_embeddings(subtractv(red_v,purple_v), color_table)[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[160.63934760823702, 'blood'],\n"," [161.48374531202822, 'dark red'],\n"," [161.67250848551834, 'mahogany'],\n"," [162.46230331987786, 'dried blood'],\n"," [163.7192719260625, 'deep brown'],\n"," [167.2154299100415, 'deep red'],\n"," [167.58878244083044, 'reddy brown'],\n"," [168.12197952677099, 'blood red'],\n"," [168.6297719858507, 'indian red'],\n"," [169.7203582367183, 'chocolate brown']]"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QlQK7FDp0fkZ"},"source":["##Step 6.\n","\n","Please answer this question:\n","\n","\n","What are the 10 colors closest to blue plus green?\n","\n"]},{"cell_type":"code","metadata":{"id":"g8ZNxA4Z0tpI","colab_type":"code","outputId":"b7906b4f-35cb-4819-fec9-f94c5e0b7a5c","executionInfo":{"status":"ok","timestamp":1589408909799,"user_tz":420,"elapsed":5785,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["#your code\n","blue_v = color_table.loc['blue'].to_list()\n","green_v = color_table.loc['green'].to_list()\n","\n","ordered_embeddings(addv(blue_v, green_v), color_table)[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[14.212670403551895, 'bright turquoise'],\n"," [15.0996688705415, 'bright light blue'],\n"," [20.73644135332772, 'bright aqua'],\n"," [27.49545416973504, 'cyan'],\n"," [33.34666400106613, 'neon blue'],\n"," [38.3275357934736, 'aqua blue'],\n"," [42.49705872175156, 'bright cyan'],\n"," [45.05552130427524, 'bright sky blue'],\n"," [49.09175083453431, 'aqua'],\n"," [56.2672195865408, 'bright teal']]"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QyBvlVrC0ubQ"},"source":["##Step 7.\n","\n","Please answer this question:\n","\n","An analogy: pink is to red as X is to blue. What are the 10 best colors to choose for X?\n","\n","This one is a little trickier. Let me see if I can restate it. I am trying to solve this equation where the tilda stands for *roughly equal to*.\n","\n","<pre>\n","(pink - red) ~ (X - blue)\n"," or\n","(pink - red) + blue ~ X\n","</pre>\n","What I get:\n","<pre>\n","[[163.29727493133498, 'neon blue'],\n"," [163.44418007380992, 'bright sky blue'],\n"," [170.0764533967004, 'bright light blue'],\n"," [172.97976760303501, 'cyan'],\n"," [174.54512310574592, 'bright cyan'],\n"," [176.39727889057698, 'bright turquoise'],\n"," [178.23860412379804, 'clear blue'],\n"," [178.54131174604942, 'azure'],\n"," [178.92456511055155, 'dodger blue'],\n"," [180.95303258028034, 'lightish blue']]\n"," </pre>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TvQz7kbG7WLs","outputId":"b9c0dfd1-d240-4ab8-d07f-3d4986d8c523","executionInfo":{"status":"ok","timestamp":1589408910223,"user_tz":420,"elapsed":6194,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["# an analogy: pink is to red as X is to blue\n","pink_v = color_table.loc['pink'].to_list()\n","\n","pinksubred = subtractv(pink_v, red_v)\n","comp_v = addv(pinksubred, blue_v)\n","\n","ordered_embeddings(comp_v,color_table)[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[163.29727493133498, 'neon blue'],\n"," [163.44418007380992, 'bright sky blue'],\n"," [170.0764533967004, 'bright light blue'],\n"," [172.97976760303501, 'cyan'],\n"," [174.54512310574592, 'bright cyan'],\n"," [176.39727889057698, 'bright turquoise'],\n"," [178.23860412379804, 'clear blue'],\n"," [178.54131174604942, 'azure'],\n"," [178.92456511055155, 'dodger blue'],\n"," [180.95303258028034, 'lightish blue']]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oNqUfy3p05mO"},"source":["##Step 8.\n","\n","Please answer this question:\n","\n","Another analogy: Navy is to blue as X is to green. What are the 10 best colors to choose for X?\n","\n","What I get:\n","<pre>\n","[[140.59160714637272, 'true green'],\n"," [143.85409274678284, 'dark grass green'],\n"," [147.770091696527, 'grassy green'],\n"," [148.82540105774956, 'racing green'],\n"," [151.07944929738127, 'forest'],\n"," [151.52887513606112, 'bottle green'],\n"," [153.4079528577316, 'dark olive green'],\n"," [153.6522046701576, 'darkgreen'],\n"," [154.042202009709, 'forrest green'],\n"," [154.52184311611094, 'grass green']]\n"," </pre>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tj-HeoKj7WLw","outputId":"50095594-9f75-4a6c-a9be-adc5ab5be06f","executionInfo":{"status":"ok","timestamp":1589408910390,"user_tz":420,"elapsed":6346,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["#Navy is to blue as X is to green\n","navy_v = color_table.loc['navy'].to_list()\n","\n","navysubblue = subtractv(navy_v,blue_v)\n","comp_v = addv(navysubblue,green_v)\n","\n","ordered_embeddings(comp_v,color_table)[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[140.59160714637272, 'true green'],\n"," [143.85409274678284, 'dark grass green'],\n"," [147.770091696527, 'grassy green'],\n"," [148.82540105774956, 'racing green'],\n"," [151.07944929738127, 'forest'],\n"," [151.52887513606112, 'bottle green'],\n"," [153.4079528577316, 'dark olive green'],\n"," [153.6522046701576, 'darkgreen'],\n"," [154.042202009709, 'forrest green'],\n"," [154.52184311611094, 'grass green']]"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AvpBcU_x5mOM"},"source":["##Step 9.\n","\n","Please answer this question:\n","\n","Throw all the colors together. Take the average of all the colors. What are the 10 colors closest to the average vector you get?\n","\n","Here is what I get:\n","<pre>\n","[[26.282651650428612, 'gunmetal'],\n"," [29.711576494319146, 'purplish brown'],\n"," [33.13272970610447, 'slate grey'],\n"," [33.7210781427746, 'charcoal grey'],\n"," [35.741354821053505, 'purple brown'],\n"," [36.241474093149755, 'dirty purple'],\n"," [37.04201818355894, 'slate'],\n"," [37.19468301309268, 'brownish purple'],\n"," [39.96387257400253, 'grape'],\n"," [45.38477473534245, 'greyish brown']]\n"," </pre>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6os-Tzen5mOU","outputId":"25240ba0-20c7-4e63-c076-4a69cac6a517","executionInfo":{"status":"ok","timestamp":1589408910862,"user_tz":420,"elapsed":6804,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["#your code\n","color_names = color_table.index.to_list()\n","\n","color_list = []\n","for i in color_names:\n","  color_list.append(color_table.loc[i].to_list())\n","\n","avg_color = meanv(color_list)\n","\n","ordered_embeddings(avg_color, color_table)[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[4.602553484191201, 'brown grey'],\n"," [21.139113217848376, 'reddish grey'],\n"," [21.428001832576594, 'brownish grey'],\n"," [24.206525784721993, 'medium grey'],\n"," [25.59966041635289, 'green grey'],\n"," [26.10522478271917, 'warm grey'],\n"," [27.96545249675546, 'dark khaki'],\n"," [30.709594237593613, 'grey green'],\n"," [32.567254561942164, 'grey/green'],\n"," [33.166059500486384, 'greeny grey']]"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"xFG_w3vmDxqL","colab_type":"text"},"source":["##Bold statement\n","\n","I claim that above demonstrates that it's possible to use math to reason about how people use language. Let's explore my claim a bit more."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WSnnTXt4BOe1"},"source":["#3. Doing bad digital humanities with color vectors\n","\n","With the tools above in hand, we can start using our vectorized knowledge of language toward academic ends. In the following example, I'm going to calculate the average color of Bram Stoker's *Dracula*.\n","\n","We will definitely need spacy so let's bring that in."]},{"cell_type":"code","metadata":{"id":"ZhifiwXfcAyd","colab_type":"code","colab":{}},"source":["import spacy\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fRx-LcbVEujw","colab_type":"text"},"source":["The following code will tell spacy to use something called a Graphical Processing Unit (GPU) if it is available. Otherwise, work without it.\n","\n","If you are so inclined, you can turn-on colab's GPU under Runtime/Change runtime type. The problem is that this will restart your kernel so will have to run all cells again above this. Instead, you could leave yourself a note to change it when you first start the notebook up. It will stay changed so only need to do it once (per notebook).\n","\n","What good is a GPU? It's kind of complicated. The short answer is that it *may* make your spacy code run faster. But you can also run without it, maybe just a tad slower."]},{"cell_type":"code","metadata":{"id":"kLkZvBG7P94x","colab_type":"code","outputId":"c1930c3a-f0fa-4ead-b118-d9f853377084","executionInfo":{"status":"ok","timestamp":1589408916862,"user_tz":420,"elapsed":12784,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["spacy.prefer_gpu()  #True if have GPU turned on, False if you just want to run normally\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"Dz-yQfqTyeMU","colab_type":"code","outputId":"68997eb4-d165-49c4-8c31-dfb0fa2ae8b3","executionInfo":{"status":"ok","timestamp":1589408944092,"user_tz":420,"elapsed":39997,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":581}},"source":["!python -m spacy download en_core_web_md"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting en_core_web_md==2.2.5\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n","\u001b[K     |████████████████████████████████| 96.4MB 1.2MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (46.1.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.6.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.18.4)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.4.5.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.9)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.6.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.1.0)\n","Building wheels for collected packages: en-core-web-md\n","  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp36-none-any.whl size=98051305 sha256=6cf576935ca6205206c72272680fff3c86f644ccf334262ee92c41feed930550\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-4hz1c4hg/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n","Successfully built en-core-web-md\n","Installing collected packages: en-core-web-md\n","Successfully installed en-core-web-md-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_md')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qD4w_Bdo-gZ1","colab_type":"code","colab":{}},"source":["import en_core_web_md\n","nlp = en_core_web_md.load()  #Gives us a way to parse text documents in one line of code. You will see in minute."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWnnKaazq-d4","colab_type":"code","colab":{}},"source":["#spnlp = TypeVar('spacy.lang.en.English')  #for type hints"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EmLhYnnvwD9m","colab_type":"code","outputId":"c8c15351-6dbc-4c9d-d4a3-d7bf0758cd9c","executionInfo":{"status":"ok","timestamp":1589408974723,"user_tz":420,"elapsed":70601,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["nlp('This is a test sentence.')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["This is a test sentence."]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"ejsO0YZSJ983","colab_type":"text"},"source":["Let's do a little exploration of what we \n","just loaded."]},{"cell_type":"code","metadata":{"id":"Lj-CntgxBEZo","colab_type":"code","outputId":"ae777647-63d1-49c6-fdbf-cebbf495f233","executionInfo":{"status":"ok","timestamp":1589408974723,"user_tz":420,"elapsed":70582,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["nlp.vocab.length  #1.3M words - think I told you this was 20K. Not sure where I got that idea!"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1340241"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"nJw-ZfyaGOPE","colab_type":"text"},"source":["Jargon alert. A \"dunder\" method (see below) is a method that starts and ends with 2 underscores. The \"d\" stands for \"double\", the \"under\" for underscores.\n","\n","Forcing you to use these semi-arcane methods is another failing of spacy, IMHO."]},{"cell_type":"code","metadata":{"id":"iHeO__XsR_Tw","colab_type":"code","outputId":"c3189deb-76e4-4863-8082-26eb42159612","executionInfo":{"status":"ok","timestamp":1589408974724,"user_tz":420,"elapsed":70568,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["nlp.vocab.__contains__('marvelous')  #dunder method for checking if word in vocab"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"NgHtepfGBXwh","colab_type":"code","outputId":"5ac70a02-1698-4ac1-c56c-ea33e64d17e5","executionInfo":{"status":"ok","timestamp":1589408974724,"user_tz":420,"elapsed":70555,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["nlp.vocab.__contains__('askfds')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":57}]},{"cell_type":"markdown","metadata":{"id":"9RrduFcCKuOS","colab_type":"text"},"source":["#4. The color of books\n","\n","Let's check out the \"color\" of some classic books. To calculate the average color of an entire book, we'll follow these steps:\n","\n","1. Parse the book text into words using spacy's `nlp` method.\n","2. Check every word to see if it names a color in our vector space, i.e., the color_table. If it does, add it to a list of vectors.\n","3. Find the average of that list of vectors.\n","4. Find the color(s) closest to that average vector.\n","\n","I'm going to set up links to 3 classic novels. All of the links point to text copies of books maintained by Project Gutenberg. If you have a favorite book that has outrun its copyright, you may find it at gutenberg.org."]},{"cell_type":"code","metadata":{"id":"hziYZ6rgSpuX","colab_type":"code","colab":{}},"source":["dracula_url = 'http://www.gutenberg.org/cache/epub/345/pg345.txt'\n","dickens_url = 'https://www.gutenberg.org/files/98/98-0.txt'  #tale of two cities\n","yellow_url = 'http://www.gutenberg.org/files/1952/1952-0.txt'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bX462dXkISHv","colab_type":"text"},"source":["These are text files and not csv files. We are going to have to go through a few more steps to load them into Python.\n","\n","First we will use a command line operation (denoted by the bang that starts it). This will bring the file into temporary storage in colab. This does **not** bring it into Google Drive. And it won't stay in temporary storage forever."]},{"cell_type":"code","metadata":{"id":"QOCvrs-DHvxa","colab_type":"code","outputId":"f0efb7e4-a8c3-465b-906d-e0488048923f","executionInfo":{"status":"ok","timestamp":1589408975898,"user_tz":420,"elapsed":71709,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["!wget {dracula_url}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-05-13 22:29:34--  http://www.gutenberg.org/cache/epub/345/pg345.txt\n","Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n","Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 883160 (862K) [text/plain]\n","Saving to: ‘pg345.txt’\n","\n","pg345.txt           100%[===================>] 862.46K  2.92MB/s    in 0.3s    \n","\n","2020-05-13 22:29:35 (2.92 MB/s) - ‘pg345.txt’ saved [883160/883160]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1kA9YZwfIyXF","colab_type":"text"},"source":["Check to make sure it is there using another command line operation ls -l."]},{"cell_type":"code","metadata":{"id":"odtNpAyGQAu8","colab_type":"code","outputId":"53082966-5fc3-4178-cb78-086fe565452c","executionInfo":{"status":"ok","timestamp":1589408976667,"user_tz":420,"elapsed":72461,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["!ls -l"],"execution_count":0,"outputs":[{"output_type":"stream","text":["total 872\n","-rw-r--r-- 1 root root 883160 May  1 07:50 pg345.txt\n","drwxr-xr-x 1 root root   4096 May  4 16:26 sample_data\n","drwxr-xr-x 4 root root   4096 May 13 22:28 uo_puddles\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NdnZ6A-iJCU1","colab_type":"text"},"source":["Now can read it into a string."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s1htTNVlgQHe","colab":{}},"source":["with open('pg345.txt', 'r') as f:\n","  dracula = f.read()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bUEq2w1bR5fJ","colab_type":"code","outputId":"4b29439a-97b0-45af-875d-5981181e9f7a","executionInfo":{"status":"ok","timestamp":1589408976669,"user_tz":420,"elapsed":72442,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["type(dracula)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["str"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"V1znKvKhJZav","colab_type":"code","outputId":"a46924aa-d94c-4bde-9a0f-b16af187f95a","executionInfo":{"status":"ok","timestamp":1589408976669,"user_tz":420,"elapsed":72428,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["dracula[:100]  #first 100 characters"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\ufeffThe Project Gutenberg EBook of Dracula, by Bram Stoker\\n\\nThis eBook is for the use of anyone anywher'"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"id":"MJ94V_0mR-m6","colab_type":"code","outputId":"e93d353e-55c4-445e-9dc1-19270235b3ab","executionInfo":{"status":"ok","timestamp":1589408976882,"user_tz":420,"elapsed":72621,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(dracula)  #867141 characters"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["867141"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"lQdTPR_yLtKt","colab_type":"text"},"source":["#5. Parse entire book\n","\n","Up until now, we have been parsing individual sentences. But spacy will handle a document that consists of multiple sentences. That document can be an article from a medical journal, a web page (after wrangling) and even an entire novel."]},{"cell_type":"code","metadata":{"id":"KAn2TRxtKJj2","colab_type":"code","colab":{}},"source":["doc = nlp(dracula.lower())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oYRRtgb1KSC6","colab_type":"text"},"source":["##We can go through doc the old fashioned way\n","\n","We can just ask for each token, one after the other. That is what we will end up doing. But wanted to show you spacy also parses out sentences."]},{"cell_type":"code","metadata":{"id":"3W3bfFkELVa4","colab_type":"code","colab":{}},"source":["drac_sentences = list(doc.sents)  #We will use this later"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AT_NmZLmLflS","colab_type":"code","outputId":"985965f1-be7e-40c1-ee34-a9bcbe670849","executionInfo":{"status":"ok","timestamp":1589408988950,"user_tz":420,"elapsed":84662,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["for i in range(5):\n","  print(i,drac_sentences[i+60])  #starting at 60th sentence to get past boilerplate"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 _)\n","\n","\n","\n","1 _3 may.\n","2 bistritz._--left munich at 8:35 p. m.\n","3 , on 1st may, arriving at\n","vienna early next morning; should have arrived at 6:46, but train was an\n","hour late.\n","4 buda-pesth seems a wonderful place, from the glimpse which i\n","got of it from the train and the little i could walk through the\n","streets.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PikNHhVTNmNl","colab_type":"code","outputId":"d6eea000-ce30-43ed-80f0-a3bdf15aec75","executionInfo":{"status":"ok","timestamp":1589408988950,"user_tz":420,"elapsed":84643,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["sentence64 = drac_sentences[64]\n","sentence64"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["buda-pesth seems a wonderful place, from the glimpse which i\n","got of it from the train and the little i could walk through the\n","streets."]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"id":"QNS7aZleNwE5","colab_type":"code","outputId":"d5bfb3f6-123e-48c3-b432-be296b897ff6","executionInfo":{"status":"ok","timestamp":1589408988951,"user_tz":420,"elapsed":84625,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":578}},"source":["for token in sentence64:\n","  print(token.text)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["buda\n","-\n","pesth\n","seems\n","a\n","wonderful\n","place\n",",\n","from\n","the\n","glimpse\n","which\n","i\n","\n","\n","got\n","of\n","it\n","from\n","the\n","train\n","and\n","the\n","little\n","i\n","could\n","walk\n","through\n","the\n","\n","\n","streets\n",".\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3EEn6Q7sz5hr","colab_type":"text"},"source":["#Now to build the matrix\n","\n","General strategy: go through every token in drac_doc and check if in color_table index. If it is, add its RGB vector to your matrix. When done, get the average RGB vector.\n","\n","I'm going to use my standard gist for this."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xG197qkQBOe5","colab":{}},"source":["#the old list is doc\n","\n","drac_color_matrix = []  #the new list\n","color_names = color_table.index.tolist()\n","\n","for token in doc:\n","  word = token.text\n","  if word in color_names:\n","    drac_color_matrix.append(color_table.loc[word].tolist())  #append the rgb values\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UK6yJZQ4S5id","colab_type":"code","outputId":"35584743-eaee-4c11-fd4a-680b6a96818c","executionInfo":{"status":"ok","timestamp":1589408991426,"user_tz":420,"elapsed":87079,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(drac_color_matrix)  #901 uses of a color word"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["901"]},"metadata":{"tags":[]},"execution_count":71}]},{"cell_type":"code","metadata":{"id":"kYRMeYPRdum4","colab_type":"code","outputId":"7a74a50c-6953-4725-8969-e6845081cd36","executionInfo":{"status":"ok","timestamp":1589408991427,"user_tz":420,"elapsed":87065,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["drac_color_matrix[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[229, 0, 0],\n"," [244, 208, 84],\n"," [255, 255, 255],\n"," [255, 255, 255],\n"," [255, 255, 255],\n"," [172, 116, 52],\n"," [0, 0, 0],\n"," [0, 0, 0],\n"," [27, 36, 49],\n"," [78, 81, 139]]"]},"metadata":{"tags":[]},"execution_count":72}]},{"cell_type":"code","metadata":{"id":"TFk_7UTwRp4P","colab_type":"code","colab":{}},"source":["avg_color = meanv(drac_color_matrix)  #your function in action!"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P41cMVPSy2B_","colab_type":"code","outputId":"aaf0e127-e940-4fdb-ab10-71023841e4ac","executionInfo":{"status":"ok","timestamp":1589408991428,"user_tz":420,"elapsed":87044,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["avg_color  #array([147.44839068, 113.65371809, 100.13540511])"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[147.44839067702551, 113.65371809100999, 100.13540510543841]"]},"metadata":{"tags":[]},"execution_count":74}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n24WAxrOBOe9"},"source":["Now, we'll pass the averaged color vector to ordering function, yielding a brown mush, which is kinda what you'd expect from adding a bunch of colors together willy-nilly."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Tz-U5U8xBOe_","outputId":"0d1b8991-f96d-4661-8757-ed7dd73bd0f9","executionInfo":{"status":"ok","timestamp":1589408991658,"user_tz":420,"elapsed":87260,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["ordered_embeddings(avg_color, color_table)[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[13.519858753013214, 'reddish grey'],\n"," [15.356247186381948, 'brownish grey'],\n"," [16.350106463486874, 'brownish'],\n"," [19.826822637698537, 'brown grey'],\n"," [21.824003657449868, 'mocha'],\n"," [26.730012587581818, 'grey brown'],\n"," [28.095953180857567, 'puce'],\n"," [28.286050911198767, 'dull brown'],\n"," [29.719493432987974, 'pinkish brown'],\n"," [31.643437130672552, 'dark taupe']]"]},"metadata":{"tags":[]},"execution_count":75}]},{"cell_type":"markdown","metadata":{"id":"QCHQOcCuYZJN","colab_type":"text"},"source":["<pre>\n","[u'reddish grey',\n"," u'brownish grey',\n"," u'brownish',\n"," u'brown grey',\n"," u'mocha',\n"," u'grey brown',\n"," u'puce',\n"," u'dull brown',\n"," u'pinkish brown',\n"," u'dark taupe']\n"," </pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r14lq-GLBOfB"},"source":["On the other hand, here's what we get when we average the colors of Charlotte Perkins Gilman's classic *The Yellow Wallpaper*.  The result definitely reflects the content of the story, so maybe we're on to something here.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KnPzQWEVSXc5","outputId":"6243ac3f-5ce8-4cd3-f191-50d0eff08f8e","executionInfo":{"status":"ok","timestamp":1589408992541,"user_tz":420,"elapsed":88124,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["!wget {yellow_url}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-05-13 22:29:51--  http://www.gutenberg.org/files/1952/1952-0.txt\n","Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n","Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 51186 (50K) [text/plain]\n","Saving to: ‘1952-0.txt’\n","\n","\r1952-0.txt            0%[                    ]       0  --.-KB/s               \r1952-0.txt          100%[===================>]  49.99K  --.-KB/s    in 0.07s   \n","\n","2020-05-13 22:29:52 (722 KB/s) - ‘1952-0.txt’ saved [51186/51186]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XW7asrqXSXc-"},"source":["Check to make sure it is there using another command line operation ls -l."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7OF94WuySXc-","outputId":"6edab91b-77e5-485a-c02c-cd0ea4ea9670","executionInfo":{"status":"ok","timestamp":1589408993486,"user_tz":420,"elapsed":88994,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!ls -l\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["total 924\n","-rw-r--r-- 1 root root  51186 Apr 18 14:40 1952-0.txt\n","-rw-r--r-- 1 root root 883160 May  1 07:50 pg345.txt\n","drwxr-xr-x 1 root root   4096 May  4 16:26 sample_data\n","drwxr-xr-x 4 root root   4096 May 13 22:28 uo_puddles\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vMJa_qcqSXdA"},"source":["Now can read it into a string."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"w5GQTaYDSXdB","colab":{}},"source":["with open('1952-0.txt', 'r') as f:\n","  yellow = f.read()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ov_6jKBGSXdE","outputId":"2fe8bc72-7fb4-4ebc-a764-c63da173b7e2","executionInfo":{"status":"ok","timestamp":1589408993488,"user_tz":420,"elapsed":88974,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["yellow[:100]  #first 100 characters"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"Project Gutenberg's The Yellow Wallpaper, by Charlotte Perkins Gilman\\n\\nThis eBook is for the use of \""]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ck8EGG87SXdG","outputId":"2c4ac00c-2b49-4631-d6b0-a26abadd0348","executionInfo":{"status":"ok","timestamp":1589408993489,"user_tz":420,"elapsed":88961,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(yellow)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50780"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5jERwFo5SXdI","colab":{}},"source":["doc = nlp(yellow.lower())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vB6b6IwuTNpt","colab":{}},"source":["\n","yellow_color_matrix = []\n","color_names = color_table.index.tolist()\n","\n","for token in doc:\n","  word = token.text\n","  if word in color_names:\n","    yellow_color_matrix.append(color_table.loc[word].tolist())\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mRq6KOrRTNpv","outputId":"3d760e7e-a22e-4e8f-945f-869434181a64","executionInfo":{"status":"ok","timestamp":1589408994294,"user_tz":420,"elapsed":89742,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(yellow_color_matrix)  #26 uses of a color word"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["26"]},"metadata":{"tags":[]},"execution_count":83}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ujplvhJzTNpy","colab":{}},"source":["avg_color = meanv(yellow_color_matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9LpRtEAVTNp1","outputId":"e1fd3d16-7c63-4bf0-ba28-482de95f1519","executionInfo":{"status":"ok","timestamp":1589408994466,"user_tz":420,"elapsed":89894,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["avg_color  #[192.0, 185.26923076923077, 48.23076923076923]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[192.0, 185.26923076923077, 48.23076923076923]"]},"metadata":{"tags":[]},"execution_count":85}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3KoZyEw4BOfD","outputId":"e9a6ec20-4fb1-451c-9fd1-b69c65cb30e2","executionInfo":{"status":"ok","timestamp":1589409083849,"user_tz":420,"elapsed":516,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["ordered_embeddings(avg_color, color_table)[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[32.867606937512456, 'pea'],\n"," [34.6139529618472, 'puke yellow'],\n"," [35.25580652163731, 'sick green'],\n"," [37.701902232548726, 'vomit yellow'],\n"," [39.06073635071867, 'booger'],\n"," [39.34720654280166, 'olive yellow'],\n"," [40.548768942125655, 'snot'],\n"," [41.77193998863593, 'gross green'],\n"," [42.05000193486195, 'dirty yellow'],\n"," [42.420635957392086, 'mustard yellow']]"]},"metadata":{"tags":[]},"execution_count":87}]},{"cell_type":"markdown","metadata":{"id":"5zA1YZGaZ1Re","colab_type":"text"},"source":["<pre>\n","[u'pea',\n"," u'puke yellow',\n"," u'sick green',\n"," u'vomit yellow',\n"," u'booger',\n"," u'olive yellow',\n"," u'snot',\n"," u'gross green',\n"," u'dirty yellow',\n"," u'mustard yellow']\n"," </pre>"]},{"cell_type":"markdown","metadata":{"id":"pUzsKlkr0uLv","colab_type":"text"},"source":["Definitely captures the yellowness (in kind of a gross way!)."]},{"cell_type":"markdown","metadata":{"id":"Y6xOJ4P8T3pJ","colab_type":"text"},"source":["#Assignment 2\n","<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n","\n","Write a function `build_embedding_matrix` that takes as parameters a string (e.g., a book) and a table (e.g., the color_table) and produces the matrix of values.\n","\n","If you are thinking about using asserts, here is one that will check to see if a variable holds a pandas table:\n","<pre>\n","assert isinstance(table, pd.core.frame.DataFrame), f'table not a dataframe but instead a {type(table)}'\n","</pre>"]},{"cell_type":"code","metadata":{"id":"Ju7j97EpUcsv","colab_type":"code","colab":{}},"source":["#your code\n","def build_embedding_matrix(string, table) -> list:\n","  assert isinstance(string, str), f'string is not a str but instead a {type(string)}'\n","  assert isinstance(table, pd.core.frame.DataFrame), f'table not a dataframe but instead a {type(table)}'\n","\n","  doc = nlp(string.lower())\n","  embedding_matrix = []\n","  table_names = table.index.tolist()\n","  for token in doc:\n","    word = token.text\n","    if word in table_names:\n","      embedding_matrix.append(table.loc[word].tolist())\n","  \n","  return embedding_matrix\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-fZTBcBPYGlQ","colab_type":"text"},"source":["Test your function out."]},{"cell_type":"code","metadata":{"id":"oTn_Hn1UVeZ8","colab_type":"code","colab":{}},"source":["yellowmat = build_embedding_matrix(yellow, color_table)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rP3jq7bDWW2r","colab_type":"code","outputId":"950a51a5-2b93-4880-bbb8-b2b730215166","executionInfo":{"status":"ok","timestamp":1589409387929,"user_tz":420,"elapsed":362,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["yellowmat == yellow_color_matrix  #check against what we did above - should be True"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":95}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4ygvjSgtYXUi","colab":{}},"source":["dracmat = build_embedding_matrix(dracula, color_table)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jkeOVdwBYXUn","outputId":"fd881b9e-f9db-42a0-8a33-89a814802718","executionInfo":{"status":"ok","timestamp":1589409407958,"user_tz":420,"elapsed":345,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["dracmat == drac_color_matrix  #check against what we did above - should be True"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":97}]},{"cell_type":"markdown","metadata":{"id":"fs8aXRR1Yk0q","colab_type":"text"},"source":["##Congrats!\n","\n","You have created a set of functions that are kind of useful."]},{"cell_type":"markdown","metadata":{"id":"vBVBeKwEaQR2","colab_type":"text"},"source":["#Assignment 3\n","<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n","\n","Put your function to use. Find the average color of A Tale of Two Cities. Use your function at the appropriate spot.\n","\n","Match my top 10:\n","<pre>\n","[[12.272728816373057, 'dark taupe'],\n"," [14.961457974171816, 'cocoa'],\n"," [16.458517047363525, 'greyish brown'],\n"," [17.920407343606808, 'dull brown'],\n"," [19.65895044742039, 'grey brown'],\n"," [21.21055110632527, 'dirt'],\n"," [24.630416790742295, 'dark mauve'],\n"," [24.877166111255395, 'dirt brown'],\n"," [29.25547441759158, 'brownish'],\n"," [29.602296007881918, 'brownish grey']]\n"," </pre>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SpPDHl2Aansb","outputId":"21273139-e418-4b54-ce97-f1fd46d2390e","executionInfo":{"status":"ok","timestamp":1589409619848,"user_tz":420,"elapsed":1520,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["#grab the file as a big string then use your functions\n","dickens_url = 'https://www.gutenberg.org/files/98/98-0.txt'  #tale of two cities\n","!wget {dickens_url}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-05-13 22:40:18--  https://www.gutenberg.org/files/98/98-0.txt\n","Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n","Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 804335 (785K) [text/plain]\n","Saving to: ‘98-0.txt’\n","\n","98-0.txt            100%[===================>] 785.48K  2.02MB/s    in 0.4s    \n","\n","2020-05-13 22:40:19 (2.02 MB/s) - ‘98-0.txt’ saved [804335/804335]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"trxktMf1Mfc_","colab_type":"code","outputId":"0b4626b3-f154-4c8b-aeb0-245be6d52914","executionInfo":{"status":"ok","timestamp":1589409624505,"user_tz":420,"elapsed":1256,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["!ls -l\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["total 1712\n","-rw-r--r-- 1 root root  51186 Apr 18 14:40 1952-0.txt\n","-rw-r--r-- 1 root root 804335 Mar 19  2018 98-0.txt\n","-rw-r--r-- 1 root root 883160 May  1 07:50 pg345.txt\n","drwxr-xr-x 1 root root   4096 May  4 16:26 sample_data\n","drwxr-xr-x 4 root root   4096 May 13 22:28 uo_puddles\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BYH5onQ5MhLP","colab_type":"code","colab":{}},"source":["with open('98-0.txt', 'r') as f:\n","  dickens = f.read()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4yOCdCzVMndB","colab_type":"code","outputId":"60d7f4cd-15c8-4960-9ff6-1db83046fc95","executionInfo":{"status":"ok","timestamp":1589409649942,"user_tz":420,"elapsed":410,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["dickens[60:100]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'rles Dickens\\n\\nThis eBook is for the use '"]},"metadata":{"tags":[]},"execution_count":101}]},{"cell_type":"code","metadata":{"id":"963-DCvONVzx","colab_type":"code","colab":{}},"source":["dickens_mat = build_embedding_matrix(dickens, color_table)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"omRHaH_SNjMK","colab_type":"code","outputId":"0fddd4fb-a687-48d6-b602-1dd172c4f284","executionInfo":{"status":"ok","timestamp":1589409892032,"user_tz":420,"elapsed":321,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["dickens_color = meanv(dickens_mat)\n","dickens_color"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[132.22535211267606, 93.2018779342723, 80.59154929577464]"]},"metadata":{"tags":[]},"execution_count":104}]},{"cell_type":"code","metadata":{"id":"vDdMSMPqNo76","colab_type":"code","outputId":"3c923869-ea0a-47fa-8ff8-15dd25a83b66","executionInfo":{"status":"ok","timestamp":1589409924848,"user_tz":420,"elapsed":451,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["ordered_embeddings(dickens_color, color_table)[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[12.272728816373057, 'dark taupe'],\n"," [14.961457974171816, 'cocoa'],\n"," [16.458517047363525, 'greyish brown'],\n"," [17.920407343606808, 'dull brown'],\n"," [19.65895044742039, 'grey brown'],\n"," [21.21055110632527, 'dirt'],\n"," [24.630416790742295, 'dark mauve'],\n"," [24.877166111255395, 'dirt brown'],\n"," [29.25547441759158, 'brownish'],\n"," [29.602296007881918, 'brownish grey']]"]},"metadata":{"tags":[]},"execution_count":105}]},{"cell_type":"markdown","metadata":{"id":"e_V9ELtGf9WL","colab_type":"text"},"source":["#What have we learned?\n","\n","One means of converting a sequence of words into a single vector is to take the average of all the individual word vectors. We are taking the entire set of color words in a book and averaging. But we can also do the same thing with smaller units, e.g., sentences, tweets.\n","\n","Let's take the next big jump and look at word meaning captured by word-embeddings."]},{"cell_type":"markdown","metadata":{"id":"YwGkqicXhoBH","colab_type":"text"},"source":["#Start here Wednesday"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"poizyunlBOfH"},"source":["#6. Distributional semantics\n","\n","In the previous section, the examples are interesting because of a simple fact: colors that we think of as similar are \"closer\" to each other in RGB vector space. In our color vector space, or in our animal cuteness/size space, you can think of the words identified by vectors close to each other as being *synonyms*, in a sense: they sort of \"mean\" the same thing.  Think of this in terms of writing, say, a search engine. If someone searches for \"mauve trousers,\" then it's probably also okay to show them results for, say,"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6kpwMphfBOfJ","outputId":"5c812a44-993b-4c46-ec35-8c24f131fdcd","executionInfo":{"status":"ok","timestamp":1589410647312,"user_tz":420,"elapsed":596,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["top10 = ordered_embeddings(color_table.loc['mauve'].tolist(), color_table)[:10]\n","for d, name in top10:  #using unpacking to get 2 separate assignments\n","    print(name + \" trousers\")\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["mauve trousers\n","dusty rose trousers\n","dusky rose trousers\n","brownish pink trousers\n","old pink trousers\n","reddish grey trousers\n","dirty pink trousers\n","old rose trousers\n","light plum trousers\n","ugly pink trousers\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kgsWNH9gBOfN"},"source":["That's all well and good for color words, which intuitively seem to exist in a multidimensional continuum of perception, and for our animal space, where we've written out the vectors ahead of time. But what about arbitrary words? Is it possible to create a vector space for all English words that has this same \"closer in space is closer in meaning\" property?\n","\n","To answer that, we have to back up a bit and ask the question: what does *meaning* mean? No one really knows, but one theory popular among computational linguists, computer scientists and other people who make search engines is the [Distributional Hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics), which states that:\n","\n","    Linguistic items with similar distributions have similar meanings.\n","    \n","What's meant by \"similar distributions\" is *similar contexts*. Take for example the following sentences:\n","\n","    It was really cold yesterday.\n","    It will be really warm today, though.\n","    It'll be really hot tomorrow!\n","    Will it be really cool Tuesday?\n","    \n","According to the Distributional Hypothesis, the words `cold`, `warm`, `hot` and `cool` must be related in some way (i.e., be close in meaning) because they occur in a similar context, i.e., between the word \"really\" and a word indicating a particular day. (Likewise, the words `yesterday`, `today`, `tomorrow` and `Tuesday` must be related, since they occur in the context of a word indicating a temperature.)\n","\n","In other words, according to the Distributional Hypothesis, a word's meaning is just a big list of all the contexts it occurs in. Two words are closer in meaning if they share contexts."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KgVJrMnQBOfQ"},"source":["#7. Word vectors by counting contexts\n","\n","So how do we turn this insight from the Distributional Hypothesis into a system for creating general-purpose vectors that capture the meaning of words?  Let's use a small source text to begin with, such as this excerpt from Dickens:\n","\n","    It was the best of times, it was the worst of times.\n","\n","This spreadsheet tries to capture the context of words. \n","![dickens contexts](http://static.decontextualize.com/snaps/best-of-times.png)\n","\n","The spreadsheet has one column for every possible context, and one row for every word. The values in each cell correspond with how many times the word occurs in the given context. The numbers in the columns constitute that word's vector, i.e., the vector for the word `of` is\n","\n","    [0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n","    \n","Because there are ten possible contexts, this is a ten dimensional space. You could use the same distance formula that we defined earlier to get useful information about which vectors in this space are similar to each other. In particular, the vectors for `best` and `worst` are actually the same (a distance of zero), since they occur only in the same context (`the ___ of`):\n","\n","    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n","    \n","Of course, the conventional way of thinking about \"best\" and \"worst\" is that they're *antonyms*, not *synonyms*. But they're also clearly two words of the same kind, with related meanings (through opposition), a fact that is captured by this distributional model.\n","\n","### Contexts and dimensionality\n","\n","In a corpus (collection of text) of any reasonable size, there will be many thousands if not many millions of possible contexts. It turns out, though, that many of the dimensions end up being superfluous and can either be eliminated or combined with other dimensions without significantly affecting the predictive power of the resulting vectors. The process of getting rid of superfluous dimensions in a vector space is called [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction), and most implementations of count-based word-vectors make use of dimensionality reduction so that the resulting vector space has a reasonable number of dimensions (say, 100—300, depending on the corpus and application).\n","\n","The question of how to identify a \"context\" is itself very difficult to answer. In the toy example above, we've said that a \"context\" is just the word that precedes and the word that follows. Depending on your implementation of this procedure, though, you might want a context with a bigger \"window\" (e.g., two words before and after), or a non-contiguous window (skip a word before and after the given word). You might look at larger syntactic structure: what are the syntactic-contexts you find the word in? You might exclude certain \"function\" words like \"the\" and \"of\" when determining a word's context, or you might [lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) the words before you begin your analysis, so two occurrences with different \"forms\" of the same word count as the same context. These are all questions open to research and debate, and different implementations of procedures for creating count-based word vectors make different decisions on this issue. In chapter 5, we eliminated stop words but we did not go as far as lemmatizing.\n","\n","### GloVe vectors\n","\n","But you don't have to create your own word vectors from scratch! Many researchers have made downloadable databases of pre-trained vectors. One such project is Stanford's [Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove/). These 300-dimensional vectors are included with spaCy, and they're the vectors we'll be using for the rest of this chapter. In fact, you already have them. They come with `en_core_web_md`. Nice.\n","\n","Check this out."]},{"cell_type":"code","metadata":{"id":"igg2SCSsB5am","colab_type":"code","outputId":"d9b42efe-f1de-4efc-c763-426d839c1b43","executionInfo":{"status":"ok","timestamp":1589411124315,"user_tz":420,"elapsed":382,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["nlp.vocab.has_vector('frankenstein')  #check to make sure word vectors have been loaded"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":107}]},{"cell_type":"code","metadata":{"id":"px9O-Qle-h21","colab_type":"code","colab":{}},"source":["dogv = nlp.vocab['dog'].vector  #get the 300d vector for dog"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cl7uiIyMAUed","colab_type":"code","outputId":"edf74128-a3a9-49ba-cc02-110c37d2da03","executionInfo":{"status":"ok","timestamp":1589411131637,"user_tz":420,"elapsed":4167,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["type(dogv)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["cupy.core.core.ndarray"]},"metadata":{"tags":[]},"execution_count":109}]},{"cell_type":"markdown","metadata":{"id":"aTUESvFobPIe","colab_type":"text"},"source":["The vector is in a peculiar spacy data type so let's just turn it into a Python list."]},{"cell_type":"code","metadata":{"id":"zph6bsOuvYQC","colab_type":"code","colab":{}},"source":["dog_list = dogv.tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lm40VLOVAX8i","colab_type":"code","outputId":"848c5ed1-b1f5-447e-e219-a8373af99b6a","executionInfo":{"status":"ok","timestamp":1589411138715,"user_tz":420,"elapsed":337,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(dog_list)  #all spacy word vectors are length 300"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["300"]},"metadata":{"tags":[]},"execution_count":112}]},{"cell_type":"code","metadata":{"id":"WIbmgPRdSWnU","colab_type":"code","outputId":"9dc94609-515e-4c59-94de-410a19a52c93","executionInfo":{"status":"ok","timestamp":1589411161938,"user_tz":420,"elapsed":414,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["dog_list[:10]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-0.4017600119113922,\n"," 0.37057000398635864,\n"," 0.02128100022673607,\n"," -0.3412500023841858,\n"," 0.04953800141811371,\n"," 0.29440000653266907,\n"," -0.17375999689102173,\n"," -0.2798199951648712,\n"," 0.06762199848890305,\n"," 2.169300079345703]"]},"metadata":{"tags":[]},"execution_count":113}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Gxq6MTMdBOfh"},"source":["For the sake of convenience, the following function gets the vector of a given string from spaCy's vocabulary:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ACYX11K2BOfi","colab":{}},"source":["def get_vec(s:str) -> list:\n","    return nlp.vocab[s].vector.tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6qLBNT-PIGIA","colab_type":"code","outputId":"5b3ea087-45b8-457c-9e45-dfb60a2ea642","executionInfo":{"status":"ok","timestamp":1589411204290,"user_tz":420,"elapsed":431,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["get_vec('dog') == dog_list  #should be the same"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"markdown","metadata":{"id":"ZdB7gAq1mfpb","colab_type":"text"},"source":["We even have a vector for words not in the vocab. It is all zeroes."]},{"cell_type":"code","metadata":{"id":"eKlPKKo-Tqbx","colab_type":"code","outputId":"b9f9b563-e1a9-4e94-a65c-2997a7d24d7b","executionInfo":{"status":"ok","timestamp":1589411207758,"user_tz":420,"elapsed":387,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["zero_vec = get_vec('askfsda')  #not in vocab\n","zero_vec.count(0)  #300 zeroes, i.e., all zeroes."],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["300"]},"metadata":{"tags":[]},"execution_count":116}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BR4DrPOPBOfp"},"source":["The following cell shows that the cosine similarity between `dog` and `puppy` is larger than the similarity between `trousers` and `octopus`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0z_UnLGmBOfq","scrolled":false,"outputId":"22c7b237-a1c5-4524-bab4-13e6d9a07408","executionInfo":{"status":"ok","timestamp":1589411213650,"user_tz":420,"elapsed":383,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["up.cosine_similarity(get_vec('dog'), get_vec('puppy')) > up.cosine_similarity(get_vec('trousers'), get_vec('octopus'))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":117}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ftIRvBDyBOgd"},"source":["#8. Sentence similarity\n","\n","I am going to switch gears a bit, and move us closer to doing prediction. What I will be interested in is converting an entire sentence into a single glove vector.  Here is general idea. I'll go through each row and grab the text of the sentence. I'll build a list of guarded tokens, kind of like e_list for Naive Bayes.\n","\n","I'll then get the vectors for all the tokens and build a matrix. I'll then take the average. I will use that vector average as the representation of the sentence.\n","\n","And as always, when I say \"I\", I mean you :)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8rDDp5zNxD7H","colab_type":"text"},"source":["##Let's work on our example sentences"]},{"cell_type":"code","metadata":{"id":"N1ZKLZtxw-hC","colab_type":"code","colab":{}},"source":["pilot_sentences = [\n","  'It was really cold yesterday.',\n","  'It will be really warm today, though.',\n","  \"It'll be really hot tomorrow!'\",\n","  'Will it be really cool Tuesday?'\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_cwe4Fz3BOge"},"source":["#Assignment 4\n","<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n","\n","Get the average vector for the first (0th) sentence in pilot_sentences. Store it in `s0_average`. Should be of length 300."]},{"cell_type":"code","metadata":{"id":"41tGzXcxD8Nq","colab_type":"code","colab":{}},"source":["#your code\n","s0 = pilot_sentences[0].lower()\n","doc = nlp(s0)\n","s0_matrix = []\n","for token in doc:\n","  if token.is_alpha and not token.is_stop:\n","    s0_matrix.append(get_vec(token.text))\n","\n","s0_average = meanv(s0_matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZaIeVwPmzo7m","colab_type":"code","outputId":"0c598e5d-7e8a-48fe-b837-8ab6358b7747","executionInfo":{"status":"ok","timestamp":1589411965748,"user_tz":420,"elapsed":374,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["len(s0_average)  #300"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["300"]},"metadata":{"tags":[]},"execution_count":124}]},{"cell_type":"markdown","metadata":{"id":"r0S75CSBchyK","colab_type":"text"},"source":["Check against my results"]},{"cell_type":"code","metadata":{"id":"ypUs335BzJl7","colab_type":"code","outputId":"10a38865-d8ca-4a00-ea2e-6adf788def46","executionInfo":{"status":"ok","timestamp":1589411969021,"user_tz":420,"elapsed":342,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(s0_average[:10])  #[0.20979999750852585, 0.25439999997615814, 0.13877900503575802, -0.01888199895620346, -0.16211500018835068, -0.1315389983355999, -0.14700499922037125, -0.09826499223709106, -0.0666164979338646, 2.3787500858306885]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0.20979999750852585, 0.25439999997615814, 0.13877900503575802, -0.01888199895620346, -0.16211500018835068, -0.1315389983355999, -0.14700499922037125, -0.09826499223709106, -0.0666164979338646, 2.3787500858306885]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IYxrDzxzzvBr","colab_type":"text"},"source":["#Assignment 5\n","<img src='https://www.dropbox.com/s/3uyvp722kp5to2r/assignment.png?raw=1' width='300'>\n","\n","I want to get the average for all the sentences in pilot_sentences. Instead of tediously copying and pasting code, please write a function `sent2vec` that takes as parameter a sentence (raw string) and produces the average glove vector. So you are packaging up the steps above.\n","\n","However, there is one twist. If you run across a sentence that adds nothing to the matrix, i.e., it has no legal tokens, then just return this value:\n","<pre>\n","[0.0]*300  #produces a list of 300 zeroes.\n","</pre>\n","That will build a 300 element list that is all 0.0."]},{"cell_type":"code","metadata":{"id":"dTRBR_Kf0LEO","colab_type":"code","colab":{}},"source":["#your code\n","def sent2vec(sentence) -> list:\n","  assert isinstance(sentence, str), f'sentence is not a str but instead a {type(sentence)}'\n","\n","  doc = nlp(sentence.lower())\n","  sentence_vectors = []\n","  for token in doc:\n","    if token.is_alpha and not token.is_stop:\n","      sentence_vectors.append(get_vec(token.text))\n","  \n","  if len(sentence_vectors) < 1:\n","    sentence_average = [0.0]*300\n","  else:\n","    sentence_average = meanv(sentence_vectors)\n","\n","  return sentence_average\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sTmPMx2k0heb","colab_type":"text"},"source":["Test it on first sentence and see if we match what we got by hand."]},{"cell_type":"code","metadata":{"id":"7QAmonXd0y08","colab_type":"code","outputId":"61973340-0955-4518-8c93-f6503cd02305","executionInfo":{"status":"ok","timestamp":1589412564422,"user_tz":420,"elapsed":385,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["s0_average == sent2vec(pilot_sentences[0])  #True"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":132}]},{"cell_type":"markdown","metadata":{"id":"W1xyJzmT-04q","colab_type":"text"},"source":["Check on weird sentence."]},{"cell_type":"code","metadata":{"id":"uwSm_aQK-3_P","colab_type":"code","outputId":"74c88f8e-bb1a-4407-b508-d6079856dbe9","executionInfo":{"status":"ok","timestamp":1589412572488,"user_tz":420,"elapsed":1643,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["sent2vec('\\n \\n')[:10]  #[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"]},"metadata":{"tags":[]},"execution_count":133}]},{"cell_type":"markdown","metadata":{"id":"QQLtu8NVI_cT","colab_type":"text"},"source":["Let's do a cross-check on how similar the pilot sentences are."]},{"cell_type":"markdown","metadata":{"id":"X16RJ8ze4DIQ","colab_type":"text"},"source":["Here they are again for reference.\n","\n","<pre>\n","0 'It was really cold yesterday.',\n","1 'It will be really warm today, though.',\n","2 \"It'll be really hot tomorrow!'\",\n","3 'Will it be really cool Tuesday?'\n","</pre>\n","\n","My results:\n","<pre>\n","0 1 0.7743651246870916\n","0 2 0.724727875606952\n","0 3 0.6154703833714615\n","1 2 0.7275724681645338\n","1 3 0.6179968179574922\n","2 3 0.6912142233533577\n","</pre>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TOfTleDWBOge","outputId":"54da63bf-6a27-4b4f-b83f-84625c412746","executionInfo":{"status":"ok","timestamp":1589412588279,"user_tz":420,"elapsed":583,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["for i in range(0, len(pilot_sentences)-1):\n","  for j in range(i+1, len(pilot_sentences)):\n","    av1 = sent2vec(pilot_sentences[i])\n","    av2 = sent2vec(pilot_sentences[j])\n","    sim = up.cosine_similarity(av1, av2)\n","    print(i, j, sim)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 1 0.7743651246870916\n","0 2 0.724727875606952\n","0 3 0.6154703833714615\n","1 2 0.7275724681645338\n","1 3 0.6179968179574922\n","2 3 0.6912142233533577\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"enFa_ghC4zC3","colab_type":"text"},"source":["They are relatively close.\n","\n","Let's add a random sentence and try again."]},{"cell_type":"code","metadata":{"id":"k-So6h6s42vX","colab_type":"code","colab":{}},"source":["pilot_sentences.append('It was the best of times and it was the worst of times.')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XWLqg0GC4_gn","colab_type":"code","outputId":"ccf8487b-9445-4870-9273-3717bdd8b6d0","executionInfo":{"status":"ok","timestamp":1589412639280,"user_tz":420,"elapsed":887,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["for i in range(0, len(pilot_sentences)-1):\n","  for j in range(i+1, len(pilot_sentences)):\n","    av1 = sent2vec(pilot_sentences[i])\n","    av2 = sent2vec(pilot_sentences[j])\n","    sim = up.cosine_similarity(av1, av2)\n","    print(i, j, sim)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 1 0.7743651246870916\n","0 2 0.724727875606952\n","0 3 0.6154703833714615\n","0 4 0.5446654613600631\n","1 2 0.7275724681645338\n","1 3 0.6179968179574922\n","1 4 0.529159756265171\n","2 3 0.6912142233533577\n","2 4 0.4915770408238424\n","3 4 0.42069515226065557\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vFEDZdbo5JpI","colab_type":"text"},"source":["The new sentence is less similar than the others, so that makes logical sense."]},{"cell_type":"markdown","metadata":{"id":"omkziCmz0l3u","colab_type":"text"},"source":["#9. Finding sentence matches in a document\n","\n","I'll create a test sentence and then look for matching sentences in Dracula. You could do the same for a document or a journal article.\n","\n","First, glove-ify all the Dracula sentences and place in a matrix.\n","\n","Warning: this took me about 5 minutes. But once I have it, I can quickly try different sentences.\n","\n","Also note that this matrix could serve as a KNN matrix. What it is missing is a label. I have no labels on the Dracula sentences. So we are just exploring our data at this point."]},{"cell_type":"code","metadata":{"id":"SDQHzP61W2EY","colab_type":"code","colab":{}},"source":["drac_matrix = []\n","\n","for i in range(len(drac_sentences)): #we defined drac_sentences above\n","  sentence = drac_sentences[i].text\n","  vec = sent2vec(sentence)\n","  drac_matrix.append(vec)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yg0OGS3KZFdb","colab_type":"code","colab":{}},"source":["sentence = drac_sentences[6].text\n","sentence_vectors = []\n","doc = nlp(sentence)\n","for token in doc:\n","  if token.is_alpha and not token.is_stop:\n","    sentence_vectors.append(get_vec(token.text))\n","\n","# take mean manually"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6P7YUVoB4xR8","colab_type":"code","colab":{}},"source":["test_sentence = \"My favorite food is strawberry ice cream.\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ORMf1S9CAACn","colab_type":"text"},"source":["Ok, find sentences in Dracula that are closest to this using sent2vec.\n"]},{"cell_type":"code","metadata":{"id":"1sgmVoowXpUy","colab_type":"code","colab":{}},"source":["input_vec = sent2vec(test_sentence)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9rMedZOy_9hL","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","ordered_distances = []\n","\n","for i in range(len(drac_matrix)):  #we defined drac_sentences above\n","  vec = drac_matrix[i]\n","  d = up.fast_cosine(np.array(input_vec), np.array(vec))  #using speedier version that relies on numpy\n","  ordered_distances.append([d, i])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a64Vie04_RoR","colab_type":"code","outputId":"7e015bab-b810-49e7-90da-5fbb6e430a36","executionInfo":{"status":"ok","timestamp":1589416245582,"user_tz":420,"elapsed":360,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"colab":{"base_uri":"https://localhost:8080/","height":561}},"source":["\n","for d,j in sorted(ordered_distances, reverse=True)[:10]:\n","  print(drac_sentences[j])\n","  print('=========')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["we get hot soup, or coffee, or tea; and\n","off we go.\n","=========\n","i had for breakfast more paprika, and a sort of porridge of maize flour\n","which they said was \"mamaliga,\" and egg-plant stuffed with forcemeat, a\n","very excellent dish, which they call \"impletata.\"\n","=========\n","this, with some cheese\n","and a salad and a bottle of old tokay, of which i had two glasses, was\n","my supper.\n","=========\n","would none of you like a cup of tea?\n","=========\n","a chicken done up some way with red pepper, which was\n","very good but thirsty.\n","=========\n","there was everywhere a bewildering mass of fruit blossom--apple,\n","plum, pear, cherry; and as we drove by i could see the green grass under\n","\n","=========\n","i dined on what they\n","called \"robber steak\"--bits of bacon, onion, and beef, seasoned with red\n","pepper, and strung on sticks and roasted over the fire, in the simple\n","style of the london cat's meat!\n","=========\n","; for there be folk that do think a balm-bowl be\n","like the sea, if only it be their own.\n","=========\n","if he can't get food\n","=========\n","come, and we'll have a cup of tea somewhere.\n","=========\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pxk9_bkotNyt","colab_type":"text"},"source":["##How about this :)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VXXBrsiZsyZf","colab":{}},"source":["test_sentence = \"The blood bank is looking for donors.\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1TKSkiLWsyZl","colab":{}},"source":["input_vec = sent2vec(test_sentence)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-yu3rwZdsyZn","colab":{}},"source":["import numpy as np\n","\n","ordered_distances = []\n","\n","for i in range(len(drac_matrix)):  #we defined drac_sentences above\n","  vec = drac_matrix[i]\n","  d = up.fast_cosine(np.array(input_vec), np.array(vec))  #using speedier version that relies on numpy\n","  ordered_distances.append([d, i])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"c3c87f4a-290f-418e-b680-270d41fe9790","executionInfo":{"status":"ok","timestamp":1589416269004,"user_tz":420,"elapsed":341,"user":{"displayName":"Lea Frank","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgOE2jMYsKFpFOM8jYIyScAm7egnmTjn2lEvC_nsg=s64","userId":"08227243406069908054"}},"id":"fppVbQ9wsyZp","colab":{"base_uri":"https://localhost:8080/","height":408}},"source":["\n","for d,j in sorted(ordered_distances, reverse=True)[:5]:\n","  print(drac_sentences[j])\n","  print('=========')\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["whereupon the captain tell him that he\n","had better be quick--with blood--for that his ship will leave the\n","place--of blood--before the turn of the tide--with blood.\n","=========\n","we must have\n","another transfusion of blood, and that soon, or that poor girl's life\n","won't be worth an hour's purchase.\n","=========\n","he had been paid for his work by\n","an english bank note, which had been duly cashed for gold at the danube\n","international bank.\n","=========\n","i don't care for the pale people; i like them with lots of blood\n","in them, and hers had all seemed to have run out.\n","=========\n","\"do you mean to tell me, friend john, that you have no suspicion as to\n","what poor lucy died of; not after all the hints given, not only by\n","events, but by me?\"\n","\n","\"of nervous prostration following on great loss or waste of blood.\"\n","\n","\"and how the blood lost or waste?\n","=========\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vL4dlbhmAJl7","colab_type":"text"},"source":["Seems kind of interesting to me. We do seem to find related sentences."]},{"cell_type":"markdown","metadata":{"id":"yjqMVNHbgWvv","colab_type":"text"},"source":["#10. Bias creeps in\n","\n","You can kind of expect it, right? If we build word-vectors from today's web content, and that web content is biased, then we will end up with bias creeping into word-vectors.\n","\n","One of the magical outcomes of word-vectors is that we can  math on them in a similar fashion to our animal and color examples.\n","\n","Here is a diagram of an example that was widely reported:\n","\n","<img src='https://www.dropbox.com/s/0norjklo12ebemj/Screenshot%202020-05-08%2011.21.29.png?raw=1'>\n","\n","In essence, find the difference between man and woman (shown as a gender distance). Then find  king and subtract the gender distance to get queen. I know you might wonder if gender and royal are really features in a word-vector, kind of like size and cuteness were for animals. Yes, but they are spread through the entire vector. So cannot point to vector[i] and say that is the gender determiner.\n"]},{"cell_type":"code","metadata":{"id":"exAcfUQTkTDc","colab_type":"code","colab":{}},"source":["woman_vec = get_vec('woman')\n","man_vec = get_vec('man')\n","king_vec = get_vec('king')\n","queen_vec = get_vec('queen')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"12dUyqf-kqdm","colab_type":"code","colab":{}},"source":["gender_dist = subtractv(man_vec, woman_vec)\n","X = subtractv(king_vec, gender_dist)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYk71F1sk70m","colab_type":"code","outputId":"34684be9-9078-4335-dd1b-94d706578598","executionInfo":{"status":"ok","timestamp":1588966341123,"user_tz":420,"elapsed":47334,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["up.cosine_similarity(X, queen_vec)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7880844327411434"]},"metadata":{"tags":[]},"execution_count":187}]},{"cell_type":"markdown","metadata":{"id":"ARxFpowLlCTu","colab_type":"text"},"source":["Fairly high as we would expect. BTW: this is pretty dang impressive if you ask me."]},{"cell_type":"markdown","metadata":{"id":"Oj3MAnV6rZXG","colab_type":"text"},"source":["\n","So what's the bias? Check another early example out.\n","\n","<img src='https://miro.medium.com/max/1400/1*DZa3CnBeyjyCrwy1wMFGNg.png'>\n","\n","In essence, if you asked for an ordering of the vectors closest to doctor, you would see \"man\" in that top 10 list but not \"woman\". The reverse for nurse.\n","\n","As I said, the word-vectors are trained on web content. They will pick up whatever bias exists in that content.\n","\n","The word-embedding gurus took so much heat from these early examples that they tried to de-bias the vectors. Here is one paper that gives a high-level overview: https://medium.com/@dhartidhami/bias-in-word-embeddings-4ce8e4261c7\n","\n","Let's see how they are doing."]},{"cell_type":"code","metadata":{"id":"X7k0r4k8lLsD","colab_type":"code","colab":{}},"source":["doctor_vec = get_vec('doctor')\n","nurse_vec = get_vec('nurse')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fSXd-ChhlmOJ","colab_type":"code","colab":{}},"source":["X = subtractv(doctor_vec, gender_dist)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rG7bcHQLl0qj","colab_type":"code","outputId":"2e6ab32c-a136-429b-ed41-43a7bc6bb8a5","executionInfo":{"status":"ok","timestamp":1588966341124,"user_tz":420,"elapsed":47298,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["up.cosine_similarity(X, nurse_vec)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7022648245931439"]},"metadata":{"tags":[]},"execution_count":190}]},{"cell_type":"markdown","metadata":{"id":"EnMHnA0Nol6e","colab_type":"text"},"source":["Still kind of a problem.\n","\n","Let's try one more thing. This uses a similarity method built-in to spacy."]},{"cell_type":"code","metadata":{"id":"ytVrza1sqGQv","colab_type":"code","outputId":"fbe42b26-07ff-4ad6-b182-a24b47e767d4","executionInfo":{"status":"ok","timestamp":1588966341261,"user_tz":420,"elapsed":47418,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["x = nlp(\"man\")\n","y = nlp(\"nurse\")\n","x.similarity(y)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(0.28819457, dtype=float32)"]},"metadata":{"tags":[]},"execution_count":191}]},{"cell_type":"code","metadata":{"id":"09HfbgdFqSDh","colab_type":"code","outputId":"1e7dc092-9d2d-47cb-ed59-f5113d6dc219","executionInfo":{"status":"ok","timestamp":1588966341262,"user_tz":420,"elapsed":47402,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["x = nlp(\"woman\")\n","y = nlp(\"nurse\")\n","x.similarity(y)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(0.49859723, dtype=float32)"]},"metadata":{"tags":[]},"execution_count":192}]},{"cell_type":"markdown","metadata":{"id":"57f2vSl2qkKx","colab_type":"text"},"source":["I think they still have a ways to go."]},{"cell_type":"markdown","metadata":{"id":"OEpKlDEL5RBu","colab_type":"text"},"source":["#Stopping here for now\n","\n","In coming weeks, I want to look at using something like `sent2vec` in a prediction model. I hope you can see we could use it with something like KNN. Take each tweet, use `sent2vec` to produce a row of 300 values, store all the rows in our crowd table. Given a new tweet, convert it using `sent2vec`, then find the k closest.\n","\n","But I want to explore using word-vectors in a new algorithm, Artificial Neural Nets (ANNs). Stay tuned."]},{"cell_type":"markdown","metadata":{"id":"nxnRcVYLUCFO","colab_type":"text"},"source":["#End notes\n","\n","For you Linguistics fans out there, here is a paper that describes the impact word-embedding is having on the Linguistic field. You may want to circle back to this after you have had some practice with it.\n","https://www.semanticscholar.org/paper/Distributional-Semantics-and-Linguistic-Theory-Boleda/510928a367d51d9ee294dd8160cc0bd66f796c60\n","\n","For you Digital Humanities fans, here is a paper that discusses the use of word-embeddings in 19th century literature. The interesting parts for me are (a) it shows why you might want to build your own word-vectors (moderately easyish) to fit text from a specific period or domain (e.g., 19th century, Medicine), and (b) why historians might want to leave biased language alone (i.e., not try to remove bias) because they want to study its evolution. Again, you might circle back to this at end of quarter.\n","http://ryanheuser.org/word-vectors/"]}]}