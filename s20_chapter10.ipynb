{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"s20_chapter10.ipynb","provenance":[{"file_id":"1CgQufQXWJzx__kq3b3oKfhnUn8h54rqm","timestamp":1591115568271},{"file_id":"1WF9dX_JcOZOS9Dh3Rr3GeRYkV-nisBKJ","timestamp":1590855812305},{"file_id":"1D_rVwtN6bpVFhzNq50yIyMGJ8zzwXtVR","timestamp":1588092424463},{"file_id":"1tW_0spm8E_mzURIjBxPKWFHbsvVcpey8","timestamp":1581894983969},{"file_id":"1suSjmwRopopnvO3L3xDxDtdllZpTM1zp","timestamp":1556465823183}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fqYN2skxBOcs"},"source":["<center><h1>Chapter 10 - Convolutional Neural Nets (CNN)</h1></center>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ye7zDEQZhd9y"},"source":["*Convolutional*. Wow, that is a mouthful. Fancy name for something that I think I can make more digestible with some pictures.\n","\n","I know I am tempting my luck by introducing yet another concept here at the end of the quarter. But I think that most of you will find a use for it in your own studies and research. So here goes."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CyYUcxgyhd93"},"source":["#1. First up: 2D CNN\n","\n","The typical problem that a 2-dimensional CNN is trying to solve is image-classification. Images are viewed as a n by m set of pixels. So in the image below, we have a picture of 3 dogs. The actual Python representation of this is as a list of lists (a matrix) of 20 by 20. Each cell in the matrix represents the color (normalized) of a pixel.\n","\n","<img src='https://miro.medium.com/max/616/1*jrnH-yMLOiSybB1u3I-OVA.png' height=300>\n"]},{"cell_type":"markdown","metadata":{"id":"0O6ViqPbdSOs","colab_type":"text"},"source":["##Plain old ANN not up to the task\n","\n","We could just flatten the 20x20 matrix into a list of 400 values. Then have an ANN with 400 input nodes. The problem is that a node may learn to spot a nose in the image above. But I could take the same image and shift the dogs to the left in it. Noses are now in a entirely new section of the image and hence are now on different input nodes.\n","\n","In general, an ANN will be terrible at this task unless we can guarantee that noses, eyes, ears are always in the same set of pixels for each new image.  Kind of a hard problem."]},{"cell_type":"markdown","metadata":{"id":"WbGWnODien1b","colab_type":"text"},"source":["##Someone had an insight: use human image classification as model\n","\n","What do we know about the visual cortex in human and animals?\n","\n","<img src='https://www.dropbox.com/s/qgal197po04v2zp/Screenshot%202020-05-29%2015.28.03.png?raw=1' height=300>\n","\n","<img src='https://www.dropbox.com/s/pzsxah4p943k2ms/Screenshot%202020-05-29%2015.28.25.png?raw=1' height=300>"]},{"cell_type":"markdown","metadata":{"id":"rNv-jWqkh4tF","colab_type":"text"},"source":["##What can we learn from this?\n","\n","We need a way to process pixels within regions. So find a way to keep the 2D matrix as input. Our input is no longer a list of numbers. It is a matrix.\n","\n","And find a way to mimic the layered architecture of the visual cortex that goes from simple feature detection to complex feature detection.\n","\n","Let's take the regions first.\n","\n","<img src='https://www.dropbox.com/s/85cstbzp3bjxyy4/Screenshot%202020-05-31%2011.07.41.png?raw=1' height=600>"]},{"cell_type":"markdown","metadata":{"id":"qXA4UIAdwYv_","colab_type":"text"},"source":["##Learning\n","\n","The filter's values are weights. They start in a random state. They are changed through backpropogation."]},{"cell_type":"markdown","metadata":{"id":"uzIVl3j60XYL","colab_type":"text"},"source":["##You can have hundreds of filters\n","\n","Both the number of filters and their dimensions and their stride are hyperparameters.\n","\n","<img src='https://www.simplilearn.com/ice9/free_resources_article_thumb/pooling.JPG' height=300>"]},{"cell_type":"markdown","metadata":{"id":"whSaGp5z1DVF","colab_type":"text"},"source":["##But wait\n","\n","Why won't the hundreds of filters just be duplicates of each other? They could be :) They do all convolve in exactly the same way.\n","\n","But highly unlikely given they all start with separate random weights. In fact, what you find is that they tend to graudally learn different patterns over training. If I take the filters output matrices and print each as an image, I may see something like this (this is for a color image).\n","\n","<img src='https://www.dropbox.com/s/oeupfxuawe5wyxw/Screenshot%202020-05-29%2013.59.16.png?raw=1' height=300>"]},{"cell_type":"markdown","metadata":{"id":"PbD4ziRr4zm6","colab_type":"text"},"source":["##We can have many layers of filters\n","\n","<img src='https://www.dropbox.com/s/zuwiiubwfyp3q1y/Screenshot%202020-05-30%2011.41.02.png?raw=1'>"]},{"cell_type":"markdown","metadata":{"id":"Z_WF1F-gz0Ec","colab_type":"text"},"source":["##Pooling\n","\n","Eventually you will want to pool. In essence, take the dominant cell within a region as the representative of that region. It also involves convolution with same hyperparameters: how many poolers, size of each pooler, etc.\n","\n","<img src='https://www.dropbox.com/s/id9bzmpnzn9dkht/Screenshot%202020-05-30%2011.19.12.png?raw=1'>"]},{"cell_type":"markdown","metadata":{"id":"a5pzuGOD6IHU","colab_type":"text"},"source":["##Final piece\n","\n","The last part of a CNN is a plain old ANN. So you flatten the matrix from the last pooling layer and just feed to an ANN like we have been studying. In the image below, the matrix we flatten is the last layer of the filtering layers.\n","\n","\n","<img src='https://i0.wp.com/vinodsblog.com/wp-content/uploads/2018/10/CNN-2.png'>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AgyQeLH-8tNV","colab_type":"text"},"source":["##Let's see it all in action!\n","\n","I am not going to try to hide the tensorflow code this time. You will need to play with it if you decide post-class that you have some images you might want to try.\n"]},{"cell_type":"code","metadata":{"id":"oSGuBgMU2jlL","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C6TMfkki9C0E","colab_type":"text"},"source":["##MNIST dataset\n","\n","What we have done above is split the MNIST dataset of 70K hand-drawn digits, 0-9. We can see that tensorflow has a built-in method to do the splitting into\n","60K training and 10K testing. Nice of tensorflow.\n","\n","The goal is to predict the digit given an image.\n","\n","I'll choose one of the images and show what it looks like in pixelalized form."]},{"cell_type":"code","metadata":{"id":"8W9LCO2B2qjz","colab_type":"code","outputId":"5eb7ecb3-6364-4a7d-cfe6-2dd637ebde18","executionInfo":{"status":"ok","timestamp":1591032049326,"user_tz":420,"elapsed":569,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":300}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","image_index = 7777 # You may select anything up to 60,000\n","print('Image label: ', y_train[image_index]) # The label is 8\n","plt.imshow(x_train[image_index], cmap='Greys')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Image label:  8\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f33f0045c88>"]},"metadata":{"tags":[]},"execution_count":15},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"LHvsF6U991zS","colab_type":"text"},"source":["##Now some wrangling\n","\n","Each image is a 28x28 set of pixels where each pixel is a value 0-255. We need to normalize. Just doing simple divide by max."]},{"cell_type":"code","metadata":{"id":"VhI4JWd73DZX","colab_type":"code","outputId":"7bd3f6d3-3fe1-4f0e-b3bd-3d0ed2a43371","executionInfo":{"status":"ok","timestamp":1591032105581,"user_tz":420,"elapsed":370,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["# Reshaping the array to 4-dims so that it can work with the Keras API\n","x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n","x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n","input_shape = (28, 28, 1)\n","# Making sure that the values are float so that we can get decimal points after division\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","# Normalizing the RGB codes by dividing it to the max RGB value.\n","x_train /= 255\n","x_test /= 255\n","print('x_train shape:', x_train.shape)\n","print('Number of images in x_train', x_train.shape[0])\n","print('Number of images in x_test', x_test.shape[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["x_train shape: (60000, 28, 28, 1)\n","Number of images in x_train 60000\n","Number of images in x_test 10000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"01iE9I9B_-c8","colab_type":"text"},"source":["##The interesting part\n","\n","Here is where we construct the net architecture. It looks a lot like what you would do with a normal ANN. But some new types of layers are now in play."]},{"cell_type":"code","metadata":{"id":"cbwo1DHI3Omj","colab_type":"code","colab":{}},"source":["# Importing the required Keras modules containing model and layers\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n","\n","# Creating a Sequential Model and adding the layers\n","model = Sequential()\n","model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n","model.add(Dense(128, activation=tf.nn.relu))\n","model.add(Dropout(0.2))\n","model.add(Dense(10,activation=tf.nn.softmax))  #output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"danroAlKAOBc","colab_type":"text"},"source":["##Model summary\n","\n","We have a single filter layer (Conv2D) that defines 28 filters of 3x3, and a single pooling layer of 2x2. That's it for the convolution piece. We flatten and then feed that into an ANN hidden layer and then into an output layer. The output layer has 10 nodes, one for each digit.\n","\n","You can see we are using RELU for hidden layer. We have seen that. What is new is Dropout layer. It's another tool to avoid overfitting.\n","\n","Also, the output layer has an activation function of softmax where we have seen sigmoid in past. Basically, it is used for multi-class problems.\n","\n","In the end, a pretty simple network architecture. Let's see how it does."]},{"cell_type":"code","metadata":{"id":"fZk1M-YF3VmN","colab_type":"code","colab":{}},"source":["model.compile(optimizer='adam', \n","              loss='sparse_categorical_crossentropy', \n","              metrics=['accuracy'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kP5W3EYiCBDA","colab_type":"text"},"source":["##More hyperparameters\n","\n","I was hiding these from you. Now they are out in the open. The loss function is kind of interesting. We have 10 output nodes and would normally compute the loss for each separately then combine. The cross-entropy idea is that you look at all 10 outputs at once and see how things are shifted around within the 10 \"bins\".\n","\n","Let's give 10 epochs a shot. See how we do."]},{"cell_type":"code","metadata":{"id":"Jb6udX9RB-NK","colab_type":"code","outputId":"a841ae81-e0e9-4f66-9da7-697f38615c54","executionInfo":{"status":"ok","timestamp":1591032304011,"user_tz":420,"elapsed":71877,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":422}},"source":["%%time\n","\n","model.fit(x=x_train,y=y_train, epochs=10)  #about a minute"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","60000/60000 [==============================] - 7s 120us/step - loss: 0.2068 - accuracy: 0.9378\n","Epoch 2/10\n","60000/60000 [==============================] - 7s 118us/step - loss: 0.0884 - accuracy: 0.9736\n","Epoch 3/10\n","60000/60000 [==============================] - 7s 117us/step - loss: 0.0596 - accuracy: 0.9813\n","Epoch 4/10\n","60000/60000 [==============================] - 7s 118us/step - loss: 0.0478 - accuracy: 0.9841\n","Epoch 5/10\n","60000/60000 [==============================] - 7s 119us/step - loss: 0.0371 - accuracy: 0.9877\n","Epoch 6/10\n","60000/60000 [==============================] - 7s 119us/step - loss: 0.0288 - accuracy: 0.9900\n","Epoch 7/10\n","60000/60000 [==============================] - 7s 119us/step - loss: 0.0278 - accuracy: 0.9906\n","Epoch 8/10\n","60000/60000 [==============================] - 7s 119us/step - loss: 0.0226 - accuracy: 0.9919\n","Epoch 9/10\n","60000/60000 [==============================] - 7s 118us/step - loss: 0.0205 - accuracy: 0.9929\n","Epoch 10/10\n","60000/60000 [==============================] - 7s 118us/step - loss: 0.0185 - accuracy: 0.9937\n","CPU times: user 1min 32s, sys: 14.5 s, total: 1min 46s\n","Wall time: 1min 11s\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7f33d9f58940>"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"fxJBaahFCbVw","colab_type":"text"},"source":["##Run on test set"]},{"cell_type":"code","metadata":{"id":"5U-QC5cO3iEU","colab_type":"code","outputId":"c5692688-a118-45eb-ab3f-cde16d6ce918","executionInfo":{"status":"ok","timestamp":1591032313777,"user_tz":420,"elapsed":1015,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["model.evaluate(x_test, y_test)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 1s 68us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.06255018095753415, 0.9847999811172485]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"vE9UchiEIWq7","colab_type":"text"},"source":["##Results\n","\n","A final loss of .057 and accuracy of .986. Caveat: I have not tried to seed the random number generators so these values change slightly everytime.\n","\n","We could get very close to 100% by exploring different values of hyperparameters."]},{"cell_type":"markdown","metadata":{"id":"QnjWGL0nChN8","colab_type":"text"},"source":["##Just to show how we did on one test image\n","\n","Notice the image is kind of sloppy. But still got correct prediction."]},{"cell_type":"code","metadata":{"id":"Yot2B9PG3tFI","colab_type":"code","outputId":"58b472ac-a784-45bf-d737-2f5411b7ea95","executionInfo":{"status":"ok","timestamp":1591032353016,"user_tz":420,"elapsed":581,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":283}},"source":["image_index = 4444\n","img_rows = 28\n","img_cols = 28\n","plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n","pred = model.predict(x_test[image_index].reshape(1, img_rows, img_cols, 1))\n","print('Prediction:', pred.argmax())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Prediction: 9\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"GPJhK0PCLIIR","colab_type":"text"},"source":["##Is that the end of the story?\n","\n","You may be saying, that's all cool, Steve, but I don't have images I am working with. I think I still have something for you :)"]},{"cell_type":"markdown","metadata":{"id":"FfAtdIEpLhdj","colab_type":"text"},"source":["#1D CNNs (Yes, 1 Dimensional!)\n","\n","Some clever person noticed that the same ideas we have been applying to 2D data, e.g., images, can apply to 1D data of a certain type. In particular, some data we might be interested in has an implied adjacency. Examples include a sentence, where the adjacency of words matter. \n","\n","<img src='https://missinglink.ai/wp-content/uploads/2019/03/1D-convolutional-example_2x.png' height=300>\n","\n","In above, think of each word represented by a 300 element embedding vector. So we have a matrix of 9x300.\n","\n","Or time-series data like the stock-market, EEGs, etc., where the adjacency is in time steps.\n","\n","<img src='https://www.dropbox.com/s/kl9c95zmlqbljab/Screenshot%202020-05-29%2015.41.33.png?raw=1' height=300>"]},{"cell_type":"markdown","metadata":{"id":"HkDygcoqP7tz","colab_type":"text"},"source":["##Example using text\n","\n","The problem is determining the sentiment of movie reviews on the IMDB web site. Each sample is a single review (multiple sentences). Example:\n","\n","<i>\n","A bright spot here or there, but not worth your time to watch. I would write a longer review, but want to lead by example; the time you spend reading this should be equal to or lesser than the time you spend watching this show.\n","</i>\n","\n","The classification is binary: 0 for negative sentiment and 1 for positive sentiment.\n","\n","This is a standard problem so built-in methods for loading the data and splitting into train and test sets."]},{"cell_type":"code","metadata":{"id":"a6Gv7busQgxz","colab_type":"code","colab":{}},"source":["max_features = 5000\n","maxlen = 400\n","batch_size = 32\n","embedding_dims = 50\n","filters = 250\n","kernel_size = 3\n","hidden_dims = 250\n","epochs = 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMDOuaEjPNAw","colab_type":"code","outputId":"43e39560-edb8-47d8-ffc0-f7f064f760da","executionInfo":{"status":"ok","timestamp":1591032624773,"user_tz":420,"elapsed":6473,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":123}},"source":["from __future__ import print_function\n","from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Activation\n","from keras.layers import Embedding\n","from keras.layers import Conv1D, GlobalMaxPooling1D\n","from keras.datasets import imdb\n","\n","print('Loading data...')\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","print(len(x_train), 'train sequences')\n","print(len(x_test), 'test sequences')\n","\n","print('Pad sequences (samples x time)')\n","x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n","print('x_train shape:', x_train.shape)\n","print('x_test shape:', x_test.shape)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Loading data...\n","25000 train sequences\n","25000 test sequences\n","Pad sequences (samples x time)\n","x_train shape: (25000, 400)\n","x_test shape: (25000, 400)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ntQIu47oQpbm","colab_type":"text"},"source":["##Build and compile the model\n","\n","Steps below are similar to our MNIST example. What is new is the Embedding layer. What this does is produce something like the spacy word-vectors (300 dim) that we have been using. However, it is not as powerful as spacy. So woud be better off bringing in spacy here and use its vectors. But trying to keep it simple(ish)."]},{"cell_type":"code","metadata":{"id":"bxoBYDauQYK8","colab_type":"code","outputId":"89843ffd-d1c5-4b51-eb29-f4909b7cde08","executionInfo":{"status":"ok","timestamp":1591032712772,"user_tz":420,"elapsed":354,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","print('Build model...')\n","model = Sequential()\n","\n","# we start off with an efficient embedding layer which maps\n","# our vocab indices into embedding_dims dimensions\n","model.add(Embedding(max_features,\n","                    embedding_dims,\n","                    input_length=maxlen))\n","model.add(Dropout(0.2))\n","\n","# we add a Convolution1D, which will learn\n","# word group filters of size filter_length:\n","model.add(Conv1D(filters,\n","                 kernel_size,\n","                 padding='valid',\n","                 activation='relu',\n","                 strides=1))\n","# we use max pooling:\n","model.add(GlobalMaxPooling1D())\n","\n","# We add a vanilla hidden layer:\n","model.add(Dense(hidden_dims))\n","model.add(Dropout(0.2))\n","\n","# We project onto a single unit output layer, and squash it with a sigmoid:\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Build model...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UEgwTTKKQ4Jm","colab_type":"text"},"source":["##Model summary\n","\n","The main bit is the use of one filter layer (Conv1D) with 250 filters of size 3, and one Pooling layer. \n","\n","For the backend ANN, we have one hidden layer of 250 nodes and an output layer.\n","Since the output is binary, we only need 1 output node and can use sigmoid."]},{"cell_type":"code","metadata":{"id":"48hRUzQ9QoOT","colab_type":"code","outputId":"5ce12963-9794-40d3-8e1c-2f6692f1c26a","executionInfo":{"status":"ok","timestamp":1591032731961,"user_tz":420,"elapsed":12926,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":178}},"source":["model.fit(x_train, y_train,\n","          batch_size=batch_size,\n","          epochs=epochs,\n","          validation_data=(x_test, y_test))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/2\n","25000/25000 [==============================] - 6s 241us/step - loss: 0.3901 - accuracy: 0.8092 - val_loss: 0.2681 - val_accuracy: 0.8862\n","Epoch 2/2\n","25000/25000 [==============================] - 6s 231us/step - loss: 0.2217 - accuracy: 0.9103 - val_loss: 0.2636 - val_accuracy: 0.8906\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7f33f0020a58>"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"KW1bWC8wPczt","colab_type":"code","outputId":"21e7ecfa-999f-48bb-937d-e9e29090d9f8","executionInfo":{"status":"ok","timestamp":1591032785854,"user_tz":420,"elapsed":1907,"user":{"displayName":"Stephen Fickas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTYV1e1vE147fD273jIgyMoLcruXBuvDZ3rnvU4Q=s64","userId":"09626902604756862380"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n","print('Test accuracy:', acc)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["25000/25000 [==============================] - 2s 63us/step\n","Test accuracy: 0.8906000256538391\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KJcFKmz2eZKG","colab_type":"text"},"source":["##89% accuracy with only 2 epochs\n","\n","Not bad."]},{"cell_type":"markdown","metadata":{"id":"cnHtEaD3magu","colab_type":"text"},"source":["##Look at 1D on time-series\n","\n","I am going to discuss a tutorial using Human Activity Recognition (HAR). I am not going to actually run the code, but instead show you snippets.\n","\n","Here is the site I am using.\n","\n","<img src='https://www.dropbox.com/s/jssm9m1e4osh3e4/Screenshot%202020-05-30%2015.00.19.png?raw=1'>"]},{"cell_type":"markdown","metadata":{"id":"WomelZ5tm08e","colab_type":"text"},"source":["##Here is what the data looks like conceptually\n","\n","<img src='https://www.dropbox.com/s/0wzfjkx3h4nsg28/Screenshot%202020-05-30%2015.03.51.png?raw=1' height=300>\n","\n","So a sample is a sequence (time-series) of 3 channels x,y,z. These 3 channels correspond to the reading obtained from a cell-phone accelerometer. The goal is to classify each sample into one of a set of activites such as walking, jogging, climbing stairs, etc."]},{"cell_type":"markdown","metadata":{"id":"Cbp_BKr7oHOf","colab_type":"text"},"source":["##The model\n","\n","As you can see in the model below, there are 2 Conv1D filter layers, back to back. Then a Pooling layer. Then one hidden layer and an output layer with a node for each type of activity.\n","\n","<pre>\n","model = Sequential()\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n","model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(100, activation='relu'))\n","model.add(Dense(n_outputs, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","</pre>\n","\n","So why did I decide not to run this? Because the data wrangling is a major pain in the butt. The HAR data is spread all over the place in multiple files. I suppose it could be converted into a google sheet form, but I did not try. Sorry.\n","\n","On the plus side, if you want to actually read these files from your local computer, you can set up google colab to do that. Or just run a notebook from your own jupyter server, which is what Jason is doing in this tutorial. Then easy-peasy to read files off your own computer.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dUjhNljpqPEW","colab_type":"text"},"source":["##The results\n","\n","Jason reports an accuracy of 91% on testing data."]},{"cell_type":"markdown","metadata":{"id":"Sjfj4Q1k1q3d","colab_type":"text"},"source":["##One more example\n","\n","<img src='https://www.dropbox.com/s/kjiea89c7tux75d/Screenshot%202020-05-31%2010.39.27.png?raw=1' height=300>\n","<hr>\n","A predictive model for the function of non-coding DNA (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4914104/). I am including this as \"sequences show up all over the place\". If you have data that has adjacency features, a 1D CNN might be for you."]},{"cell_type":"markdown","metadata":{"id":"3GrJ2FZLvpIj","colab_type":"text"},"source":["#What more do you need?\n","\n","If you have images that need classification, then there are lots of online tutorials. The biggest issue may be wrangling your images to make sure they are in a form that a 2D CNN is happy with.\n","\n","If you have text or time-series data, the path is a bit more challenging. This is a very active research area. Things don't stay still for long. To give a plug to Jason, he has a book out on using machine learning for time-series data that is aimed at the beginner. If is relatively recent.\n","\n","I should also note that the two examples I showed with 1D data are perhaps the easiest. Given a fixed sequence of data (e.g., a movie review, a time-slice of accelerometer data), classify/label it in some way. What has gained attention recently is not classifying what we have seen but instead predicting what comes next. If I give my model the start of a sentence, predict what will be the next word. If I give my model accelerometer data, predict what the user will do next. This brings in a new type of model called a \"recurrent neural net\" or RNN. It is the type of model that makes headlines by winning games (Go, Chess, Smash Brothers). It is used by Autonomous cars to predict what others on the road will do."]}]}